Directory structure:
‚îî‚îÄ‚îÄ codesprout/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ .pre-commit-config.yaml
    ‚îú‚îÄ‚îÄ docs/
    ‚îÇ   ‚îú‚îÄ‚îÄ PRD.md
    ‚îÇ   ‚îú‚îÄ‚îÄ RELEASING.md
    ‚îÇ   ‚îú‚îÄ‚îÄ SECURITY.md
    ‚îÇ   ‚îî‚îÄ‚îÄ TASKS.md
    ‚îú‚îÄ‚îÄ scripts/
    ‚îÇ   ‚îú‚îÄ‚îÄ README.md
    ‚îÇ   ‚îî‚îÄ‚îÄ sign_releases.py
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ bundler.rs
    ‚îÇ   ‚îú‚îÄ‚îÄ main.rs
    ‚îÇ   ‚îî‚îÄ‚îÄ parser.rs
    ‚îú‚îÄ‚îÄ tests/
    ‚îÇ   ‚îî‚îÄ‚îÄ integration_tests.rs
    ‚îú‚îÄ‚îÄ .cursor/
    ‚îú‚îÄ‚îÄ .github/
    ‚îÇ   ‚îú‚îÄ‚îÄ CODEOWNERS
    ‚îÇ   ‚îú‚îÄ‚îÄ copilot-instructions.md
    ‚îÇ   ‚îú‚îÄ‚îÄ dependabot.yml
    ‚îÇ   ‚îî‚îÄ‚îÄ workflows/
    ‚îÇ       ‚îú‚îÄ‚îÄ ci.yml
    ‚îÇ       ‚îú‚îÄ‚îÄ codeql.yml
    ‚îÇ       ‚îú‚îÄ‚îÄ dependency-review.yml
    ‚îÇ       ‚îú‚îÄ‚îÄ release.yml
    ‚îÇ       ‚îî‚îÄ‚îÄ scorecards.yml
    ‚îî‚îÄ‚îÄ .roo/

================================================
File: README.md
================================================
# sprout üå±

![License](https://img.shields.io/github/license/nightconcept/codesprout)
![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/nightconcept/codesprout/ci.yml)
[![Coverage Status](https://coveralls.io/repos/github/nightconcept/codesprout/badge.svg?branch=main)](https://coveralls.io/github/nightconcept/codesprout?branch=main)
![GitHub last commit](https://img.shields.io/github/last-commit/nightconcept/codesprout)
[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/nightconcept/codesprout/badge)](https://scorecard.dev/viewer/?uri=github.com/nightconcept/codesprout)

## üåü Overview

`codesprout` is a command-line interface that takes a bundle created by [gitingest](https://gitingest.com/) and sprouts it in the target directory.

`codesprout` is written in Rust for fun and learning.

## üöÄ Getting Started

### Prerequisites

*   **Rust:** Ensure you have Rust installed. You can get it from [rust-lang.org](https://www.rust-lang.org/). `codesprout` is built with the latest stable Rust version.

### Building `codesprout`

1.  **Clone the repository (if you haven't already):**
    ```bash
    git clone <repository-url>
    cd codesprout
    ```
2.  **Build for debugging:**
    ```bash
    cargo build
    ```
    The executable will be located at `target/debug/sprout`.

3.  **Build for release (optimized):**
    ```bash
    cargo build --release
    ```
    The executable will be located at `target/release/sprout`.

## üõ†Ô∏è Usage

The `sprout` CLI tool takes a bundle file as input and creates the files and directories in a specified output location.

### Command Syntax:

```bash
sprout [BUNDLE_FILE_PATH] [OUTPUT_DIRECTORY_PATH]
```

Or using flags:

```bash
sprout --input <BUNDLE_FILE_PATH> --output <OUTPUT_DIRECTORY_PATH>
```

### Arguments & Options:

*   `BUNDLE_FILE_PATH`: (Positional or via `-i`/`--input`) Path to the bundle file. This is **required**.
*   `OUTPUT_DIRECTORY_PATH`: (Positional or via `-o`/`--output`) Path to the directory where files will be sprouted.
    *   Defaults to the current working directory if not specified.
*   `-f`, `--force`: (Optional) If specified, `sprout` will overwrite any existing files in the output directory that conflict with files from the bundle. Without this flag, `sprout` will abort if any collisions are detected.

target/tarpaulin

## üîÆ Future Ideas

While the current prototype is focused and functional, here are some ideas for future enhancements:

*   **Reverse Operation ("Bundling"):** Create a bundle file from an existing directory.
*   **Overwrite Options:** Add flags like `--skip` or interactive prompts for overwriting files (complementary to the implemented `--force` flag).

## üìú License

This project is licensed under the MIT License. See [LICENSE](docs/LICENSE) for details.



================================================
File: LICENSE
================================================
MIT License

Copyright (c) 2025 Danny Solivan

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
File: .pre-commit-config.yaml
================================================
repos:
  - repo: local
    hooks:
      - id: rustfmt-check
        name: rustfmt (check)
        entry: cargo fmt --all -- --check
        language: system
        types: [rust]
        pass_filenames: false
        verbose: true
      - id: clippy-check
        name: clippy (check)
        entry: cargo clippy --all-targets -- -D warnings
        language: system
        types: [rust]
        pass_filenames: false
        verbose: true
  - repo: https://github.com/gitleaks/gitleaks
    rev: v8.16.3
    hooks:
      - id: gitleaks
  - repo: https://github.com/pylint-dev/pylint
    rev: v2.17.2
    hooks:
      - id: pylint



================================================
File: docs/PRD.md
================================================
# codesprout - Product Requirements Document (Prototype)

## 1. Introduction

- **Project Idea:** `codesprout` is a project to develop a command-line interface (CLI) utility named `sprout`. The `sprout` tool's primary function is to parse a single, consolidated text file (referred to as a "bundle file," formatted like the provided `digest.txt` example) and "sprout" its contents into an organized directory structure with corresponding files.
- **Problem/Need:** Developers, educators, and technical writers often need to share or archive sets of code files or project snippets. Traditional methods like zip archives are not always convenient, especially for embedding into documents, wikis, or systems that primarily accept plain text. A single-file representation simplifies copy-pasting, sharing in restrictive environments, and creating self-contained examples.
- **Prototype Goal:** To build a functional `sprout` CLI in Rust that can reliably parse a `digest.txt`-style bundle file, validate its format, and recreate the described directory structure and files in a specified output location. The prototype must include clear error reporting for format issues and abort on any potential file collisions in the output directory.

## 2. Core Features / User Stories

- **Feature 1: Sprout from Bundle**
    - Description: The `sprout` CLI will read a specified bundle file (formatted like `digest.txt`), parse its content to identify individual files and their target paths, and then create these files and any necessary parent directories in the designated output location.
    - User Action(s): The user executes the `sprout` command, providing the path to the bundle file and an optional output directory.
    - Outcome(s):
        - If the bundle file is valid and no file collisions are detected in the output path, the specified directory structure and files are created as per the bundle file's content. A success message is displayed.
        - If the bundle file contains format errors, all errors are reported to the user, and no file system modifications are made.
        - If creating any file would overwrite an existing file or directory in the output path, the operation is aborted before any files are written, and an error message is displayed.
    - Command: `sprout`
    - Key Inputs:
        - Bundle File Path (required): Provided as a positional argument or via `-i <path>` / `--input <path>`.
        - Output Directory Path (optional): Provided as a positional argument or via `-o <path>` / `--output <path>`. Defaults to the current working directory.
    - Expected Output:
        - Console messages indicating files being processed (optional, perhaps under a verbose flag in the future, but basic status for prototype).
        - Clear success message upon completion.
        - Detailed error messages if bundle file validation fails or if file collisions are detected.
        - The recreated files and directories in the specified output location.

## 3. Technical Specifications

- **Primary Language(s):** Rust (latest stable version available at the time of development, e.g., 1.7X.X).
- **Key Frameworks/Libraries:**
    - `clap`: For parsing CLI arguments.
    - `anyhow`: (Recommended) For application-level error handling and reporting.
- **Database (if any):** None for this prototype.
- **Key APIs/Integrations (if any):** None.
- **Deployment Target (if applicable for prototype):** Local native executable for common desktop platforms (Linux, macOS, Windows).
- **High-Level Architectural Approach:**
    - A CLI application built in Rust.
    - Core logic will be separated into modules:
        - One module for parsing the `digest.txt`-style bundle file.
        - One module for handling file system operations (directory creation, file writing, collision detection).
        - Error handling will be centralized, potentially using custom error types or `anyhow` for context.
    - The process flow will be: 1. Parse arguments. 2. Read and fully validate the input bundle file. 3. Check for output collisions. 4. If all checks pass, create directories and files.
- **Critical Technical Decisions/Constraints:**
    - The input bundle file format is strictly the `digest.txt` style (multi-file concatenation with `================================================\nFile: path/to/file.ext\n================================================\n...content...` delimiters).
    - The `sprout` tool will perform a full analysis of the input bundle file for any format errors before attempting any file system modifications. If errors are found, they will be reported, and the tool will exit.
    - The tool will check for potential file/directory collisions in the target output directory before writing any files. If any collision is detected, the operation will be aborted with an error message, and no files will be written.

## 4. Project Structure (Optional)

A standard Rust binary (application) project structure will be used, generated initially by `cargo new sprout --bin`.

```
/codesprout_project_root
  ‚îú‚îÄ‚îÄ .git/
  ‚îú‚îÄ‚îÄ .github/              # For GitHub Actions, issue templates, etc.
  ‚îú‚îÄ‚îÄ docs/
  ‚îÇ   ‚îú‚îÄ‚îÄ PRD.md            # This document
  ‚îÇ   ‚îî‚îÄ‚îÄ TASKS.md          # Task list
  ‚îú‚îÄ‚îÄ src/
  ‚îÇ   ‚îú‚îÄ‚îÄ main.rs           # CLI entry point, argument parsing (clap), main logic flow
  ‚îÇ   ‚îú‚îÄ‚îÄ parser.rs         # Module for parsing the bundle file
  ‚îÇ   ‚îú‚îÄ‚îÄ bundler.rs        # Module for file/directory creation and output logic (renamed from sprouter for clarity)
  ‚îÇ   ‚îî‚îÄ‚îÄ error.rs          # (Optional) Module for custom error types if not solely relying on anyhow
  ‚îú‚îÄ‚îÄ Cargo.toml            # Rust project manifest, dependencies
  ‚îú‚îÄ‚îÄ Cargo.lock            # Generated lockfile
  ‚îú‚îÄ‚îÄ README.md             # Project README
  ‚îî‚îÄ‚îÄ target/               # Build artifacts (ignored by git)
```

- `src/`: Contains all Rust source code.
    - `main.rs`: Handles CLI argument parsing using `clap` and orchestrates the overall process.
    - `parser.rs`: Responsible for reading and validating the `digest.txt`-style bundle file format.
    - `bundler.rs`: Handles the creation of directories and files based on the parsed bundle, including collision checks.
- `docs/`: Contains project documentation.
- `Cargo.toml`: Defines project metadata, dependencies (like `clap`, `anyhow`), and profiles (e.g., for release optimization).

## 5. File Descriptions (If applicable)

- **Input Bundle File (e.g., `my_bundle.txt`, `project.digest`)**:
    - Purpose: A single text file containing the content of multiple source files or text-based project structures, along with their intended relative paths.
    - Format: Plain text. Each embedded file is demarcated by a header `================================================\nFile: path/to/file.ext\n================================================\n` followed by its content. The `path/to/file.ext` specifies the relative path where the file should be created in the output directory.
    - Key Contents: A sequence of file path specifications and their corresponding multi-line text content.

## 6. Future Considerations / Out of Scope (for this prototype)

- **Out of Scope for Prototype:**
    - **Reverse Operation ("Bundling"):** Creating a bundle file from an existing directory structure.
    - **Advanced Overwrite Options:** No `--force` flag or interactive prompts to overwrite files. The prototype will only abort on collision.
    - **Configuration File:** No external configuration for `sprout` (e.g., to customize delimiters or behavior).
    - **Ignore Patterns:** No functionality to ignore specific files or patterns during a (future) bundling operation.
    - **Complex Format Validation Beyond Basic Structure:** While basic structural validation (presence of delimiters, parsable paths) is in scope, deep semantic analysis of the content within files is not.
    - **Watching files or live updates.**
- **Potential Future Enhancements (Post-Prototype):**
    - Implement the reverse "bundling" operation.
    - Add file overwrite protection options (`--force`, skip, prompt).
    - Introduce a configuration file for `sprout`.
    - Support for ignore patterns (like `.gitignore`) during bundling.
    - Stricter validation options for bundle files.
    - Support for different bundle formats or custom delimiters via configuration.

## 7. Project-Specific Coding Rules (Optional)

- **Language Version:** Rust (latest stable version at the time of development).
- **Formatting:** `rustfmt` is mandatory. Code should be formatted using `cargo fmt` before committing.
- **Linting:** `clippy` is mandatory. Code should pass `cargo clippy --all-targets -- -D warnings` (fail on warnings).
- **Error Handling:**
    - Use Rust's standard `Result<T, E>` for all functions that can produce an error.
    - Utilize the `?` operator for concise error propagation.
    - Employ the `anyhow` crate for creating and managing application-level errors, providing context, and simplifying error returns from `main()`.
- **Dependencies (`Cargo.toml`):**
    - `clap` will be used for CLI argument parsing.
    - `anyhow` will be used for error handling.
    - Other external crate dependencies should be minimized and require justification for inclusion in the prototype.
- **Testing:**
    - Unit tests for the `parser.rs` module to ensure correct parsing of various valid and invalid bundle file scenarios.
    - Unit tests for the `bundler.rs` module to ensure correct file/directory creation, path handling, and collision detection logic.
    - Integration-style tests (CLI tests) to verify the overall behavior of `sprout` with sample bundle files and output directory states.
- **Naming Conventions:** Adhere to standard Rust naming conventions:
    - `snake_case` for functions, methods, variables, and modules.
    - `PascalCase` (or `CamelCase`) for types, structs, enums, and traits.
    - `UPPER_SNAKE_CASE` for constants.
- **Binary Size:** Strive for a reasonably small binary size for the release executable by configuring the release profile in `Cargo.toml` (e.g., `opt-level = "z"`, `lto = true`, `codegen-units = 1`, `panic = "abort"`, `strip = true`) and/or using `strip` utility post-compilation.
- **Comments:** Write clear comments for complex logic, public API functions, and any non-obvious decisions. Doc comments (`///`) for public items are encouraged.
- **Modularity:** Keep functions small and focused. Modules should have clear responsibilities.



================================================
File: docs/RELEASING.md
================================================
# Releasing `codesprout`

This document outlines the process for creating official releases and pre-releases for the `codesprout` project.

## Automated Release Process Overview

This project uses `release-please-action` to automate releases. When commits adhering to the [Conventional Commits](https://www.conventionalcommits.org/) specification are merged into the `main` branch, `release-please` will:

1.  **Determine the next semantic version** based on the commit messages (e.g., `fix:` triggers a patch, `feat:` triggers a minor).
2.  **Generate a changelog** from these commit messages.
3.  **Create a pull request** (or directly create a release, depending on configuration) proposing these changes.
    *   If a pull request is created, it will update `Cargo.toml` with the new version and include the generated changelog.
    *   Merging this pull request will trigger the actual release.
4.  **Create a Git tag** for the new version (e.g., `v0.2.0`).
5.  **Publish a GitHub Release** with the generated changelog and compiled binaries for Linux, macOS (x86_64 and aarch64), and Windows.

This process is documented in the main [README.md](../README.md#Î¶¥Î¶¨Ïä§-ÌîÑÎ°úÏÑ∏Ïä§-release-process-üì¶).

## Creating Pre-Releases (e.g., Alpha, Beta)

To create a specific pre-release version (e.g., `0.1.0-alpha.1`), you need to use a special commit message footer.

### Steps:

1.  **Prepare Your Code:**
    *   Ensure all code changes intended for the pre-release are committed to your local `main` branch (or the feature branch you will merge into `main`).

2.  **Craft a Special Commit:**
    *   Create a new commit (or amend an existing one if it's the last commit and not yet pushed).
    *   The commit message **must** follow the [Conventional Commits](https://www.conventionalcommits.org/) standard.
    *   Include a `Release-As:` footer in the commit message body, specifying the exact pre-release version.

    **Example Commit Message for `0.1.0-alpha.1`:**
    ```
    feat: Implement initial features for alpha release

    This commit includes the core functionality planned for the first alpha
    version of 0.1.0. It's ready for preliminary testing.

    Release-As: 0.1.0-alpha.1
    ```
    *   The `feat:` prefix (or `fix:`, `chore:`, etc.) is important for `release-please` to categorize the changes in the changelog.
    *   The `Release-As: 0.1.0-alpha.1` line explicitly tells `release-please` to create this specific version, overriding its normal version calculation for this instance.

3.  **Push to `main`:**
    *   Push this commit to the `main` branch on GitHub.
    ```bash
    git push origin main
    ```

4.  **`release-please` Takes Over:**
    *   The push to `main` will trigger the [`.github/workflows/release.yml`](../.github/workflows/release.yml) workflow.
    *   `release-please-action` will read the commit history and detect the `Release-As:` footer.
    *   It will then proceed to:
        *   Update the version in `Cargo.toml` to the specified pre-release version (e.g., `0.1.0-alpha.1`).
        *   Generate or update a `CHANGELOG.md` file.
        *   Commit these file changes directly to the `main` branch.
        *   Create a Git tag (e.g., `v0.1.0-alpha.1`).
        *   Create a GitHub Release with the corresponding title and changelog notes.
        *   The `build-and-upload-assets` job in the workflow will then build the binaries and upload them to this GitHub Release.

### Important Considerations for Pre-Releases:

*   **Base Version in `Cargo.toml`:** `release-please` uses the version in your `Cargo.toml` as a starting point. Ensure it's at a suitable base (e.g., `0.1.0` if you're creating `0.1.0-alpha.1`) before `release-please` runs.
*   **Subsequent Pre-Releases:** To release `0.1.0-alpha.2`, make further commits and then create another commit with the footer `Release-As: 0.1.0-alpha.2`.
*   **Exiting Pre-Release to Stable:** When you're ready to release the stable version (e.g., `0.1.0` after a series of alphas/betas), make a commit with the footer `Release-As: 0.1.0`. For example:
    ```
    feat: Finalize features for 0.1.0

    All planned features for the 0.1.0 release are complete and tested.
    This version is ready for stable release.

    Release-As: 0.1.0
    ```

This `Release-As` footer provides precise control over the version number, which is essential for managing pre-release cycles.



================================================
File: docs/SECURITY.md
================================================
# Security Policy

The Codesprout team and community take the security of our software seriously. We appreciate your efforts to responsibly disclose your findings, and we will make every effort to acknowledge your contributions.

## Reporting a Vulnerability

If you believe you have found a security vulnerability in Codesprout, please report it to us as soon as possible. We ask that you do not disclose the vulnerability publicly until we have had a chance to address it.

Please report vulnerabilities via one of the following methods:

* **Email:** Send an email to `[dark@nightconcept.net](mailto:dark@nightconcept.net)` with a detailed description of the vulnerability, steps to reproduce it, and any potential impact.
* **Issue Tracker (Private):** If you prefer, you can report the vulnerability through our private issue tracker `[here](https://github.com/nightconcept/codesprout/security/advisories)`.

We aim to acknowledge receipt of your vulnerability report within **3 business days**.

## Disclosure Policy

Our goal is to address and fix any reported security vulnerability in a timely manner. Here is our general process:

1. **Confirmation:** We will confirm the vulnerability and determine its impact. We may contact you for more information during this phase. This typically takes up to **7 days**.
2. **Remediation:** Our team will work on a fix for the vulnerability. The timeline for this can vary depending on the complexity of the vulnerability, but we aim to have a patch ready within **30 days** of confirmation. For more complex issues, this might extend up to **90 days**.
3. **Disclosure:** Once the vulnerability is fixed and a new version is released, we will make a public disclosure. This disclosure will typically include a description of the vulnerability and credit to the reporter, unless you request to remain anonymous. We believe in transparent disclosure practices.

We are committed to a coordinated vulnerability disclosure process. We expect to work closely with the reporter throughout the lifecycle of the vulnerability.

## Scope

This policy applies to the latest stable release of Codesprout. If you are using an older version, please consider upgrading before reporting a vulnerability, as it may have already been addressed.

## Out of Scope

The following are generally considered out of scope for our vulnerability disclosure program:

* Denial of service attacks that require significant volumetric resources.
* Social engineering or phishing attacks.
* Vulnerabilities in third-party dependencies (please report those to the respective projects, though we appreciate a heads-up if it impacts Codesprout).

Thank you for helping keep Codesprout secure. Your efforts in responsible disclosure are highly valued.



================================================
File: docs/TASKS.md
================================================
# codesprout - Task List

## Milestone 0: Development Environment & Workflow Setup

**Goal:** Establish a consistent Rust development environment using `mise` and define commands/scripts for essential development tasks such as formatting, linting, building, and testing the `sprout` CLI.

- [x] **Task 0.1: Setup `mise` for Rust Version Management**
  - [x] Create a `mise.toml` file in the project root.
  - [x] Specify the Rust version to be used for the project (e.g., `rust = "latest"` or a specific version like `rust = "1.87"` to align with PRD).
  - [x] Verification: After navigating to the project directory in a new terminal, `mise current rust` (or `mise which rustc`) shows the correct Rust compiler path and version as specified in `.mise.toml`. Running `rustc --version` confirms the active version.

- [x] **Task 0.2: Define Code Formatting Task & Integration**
  - [x] Document the command for checking formatting: `cargo fmt --all --check`.
  - [x] Document the command for applying formatting: `cargo fmt --all`.
  - [x] (Optional) Consider integrating `cargo fmt --all --check` into a pre-commit hook or CI step later.
    - [x] Verification: `cargo fmt --all --check` passes on a cleanly formatted codebase. `cargo fmt --all` correctly formats any misformatted Rust files.
- [x] **Task 0.3: Define Code Linting Task & Integration**
  - [x] Document the command for linting: `cargo clippy --all-targets -- -D warnings` (treat all warnings as errors).
  - [x] (Optional) Consider integrating this lint check into a pre-commit hook or CI step later.
  - [x] Verification: `cargo clippy --all-targets -- -D warnings` passes on a lint-free codebase.
- [ ] **Task 0.4: Define Building Tasks**
  - [x] Document command for debug builds: `cargo build`.
  - [ ] Document command for optimized release builds: `cargo build --release`.
  - [x] Note: `Cargo.toml` should be configured with release profile optimizations as per PRD (e.g., `opt-level = "z"`, `lto = true`, `panic = "abort"`, `strip = true`).
  - [x] Verification: `cargo build` successfully compiles the project. `cargo build --release` successfully compiles the project and produces an optimized binary.
- [x] **Task 0.5: Define Testing Task**  
    - [x] Document command for running all tests: `cargo test`.
    - [x] Document command for running tests with more output: `cargo test -- --nocapture` (if needed for debugging).
    - [x] Verification: `cargo test` runs all available tests and reports pass/fail status (initially, this might be an empty test suite or auto-generated tests).

## Milestone 1: Project Initialization & CLI Argument Parsing

**Goal:** Initialize the Rust project structure for `sprout` (if not fully covered by Milestone 0 tasks related to `cargo new`), implement robust command-line argument parsing using `clap`, and set up the basic application structure to handle the main processing flow.

- [x] **Task 1.1: Complete Rust Project Initialization**
    
    - [x] Run `cargo new sprout --bin` (if not done as part of `mise` setup or if a fresh start is preferred).
    - [x] Ensure `Cargo.toml` is correctly configured:
        - Set `name = "sprout"` (as the binary will be `sprout`), `description = "A CLI tool to sprout files from a bundle."`, `authors = ["Your Name <you@example.com>"]`, `edition = "2024"`, `version = "0.1.0"`.
        - Add `clap` as a dependency with the "derive" feature (e.g., `clap = { version = "4.x", features = ["derive"] }`).
        - Add `anyhow` as a dependency (e.g., `anyhow = "1.x"`).
    - [x] Verification: Project compiles successfully (`cargo build`). `Cargo.toml` reflects the specified settings and dependencies. `sprout --version` (once version is integrated with `clap`'s `App::version`) works.
- [x] **Task 1.2: Implement CLI Argument Parsing with `clap`**
    
    - [x] In `src/main.rs`, define a struct (e.g., `CliArgs`) using `clap::Parser` to manage command-line arguments.
    - [x] Implement parsing for:
        - `bundle_file_path` (positional, `PathBuf`, required unless `-i` is used).
        - `output_directory_path` (positional, `PathBuf`, optional, defaults to current directory unless `-o` is used).
        - `-i, --input <PATH>`: Optional flag for bundle file path (`Option<PathBuf>`).
        - `-o, --output <PATH>`: Optional flag for output directory path (`Option<PathBuf>`).
    - [x] Logic to determine effective input and output paths (handling defaults and overrides).
    - [x] Ensure input path is effectively mandatory.
    - [x] In `main()`, parse the arguments using `CliArgs::parse()`.
    - [x] Verification:
        - `sprout --help` displays correctly formatted help message with all arguments and options.
        - `sprout valid_bundle.txt` correctly identifies `valid_bundle.txt` as input and uses `.` as default output.
        - `sprout -i valid_bundle.txt -o ./my_output` correctly parses input and output paths.
        - `sprout ./my_output_dir` (assuming `my_output_dir` is not the bundle file) is handled by clap for positional args.
        - `sprout` (with no input arguments) shows an error message from `clap` indicating missing required input.
- [x] **Task 1.3: Establish Main Application Logic Flow & Error Handling**
    
    - [x] In `src/main.rs`, ensure `main` function returns `anyhow::Result<()>`.
    - [x] Define the high-level steps based on parsed arguments:
        1. Resolve final input and output paths.
        2. (Stub) Call `parser::process_bundle_file(input_path)`.
        3. (Stub) If parsing/validation successful, call `bundler::create_files(parsed_data, output_path)`.
    - [x] Implement basic stubs for these functions in their respective (future) modules (`parser.rs`, `bundler.rs`).
    - [x] Ensure `main` propagates errors from these calls using `?`.
    - [x] Verification: The `sprout` command runs, prints placeholder messages for each stubbed step based on parsed arguments, and exits gracefully (or with a placeholder error from a stub).

## Milestone 2: Bundle File Parsing and Validation

**Goal:** Implement the complete logic for reading, parsing, and validating the `digest.txt`-style bundle file. The tool should be able to identify all format errors in the bundle file before any file system operations are attempted.

- [x] **Task 2.1: Develop Bundle File Parser (`src/parser.rs`)**
    
    - [x] Create the `src/parser.rs` module. (Covered by previous tasks or implicitly by creating the file)
    - [x] Define a public function, e.g., `parse_bundle(bundle_path: &Path) -> anyhow::Result<Vec<ParsedEntry>>`.
    - [x] Implement logic to read the content of the bundle file.
    - [x] Implement parsing logic to iterate through the file content, recognizing the `================================================\nFile: path/to/file.ext\n================================================\n...content...` structure.
    - [x] Extract the relative file path (`String` or `PathBuf`) and the multi-line content (`String`) for each entry.
    - [x] Store the parsed data in a struct, e.g., `pub struct ParsedEntry { pub path: PathBuf, pub content: String }`.
    - [x] Verification: Unit tests for the `parser::parse_bundle` function covering:
        - [x] Empty bundle file (should return empty Vec or appropriate error).
        - [x] Bundle file with one entry.
        - [x] Bundle file with multiple entries.
        - [x] Entries with empty content.
        - [x] Entries with complex multi-line content.
        - [x] Correct path extraction (including paths with subdirectories).
- [x] **Task 2.2: Implement Bundle File Format Validation**
    
    - [x] Within `src/parser.rs`, enhance the parsing or add a distinct validation step for the parsed entries and the overall bundle structure. This validation should occur before returning successfully from `parse_bundle`.
    - [x] Validation checks should include:
        - Each `File:` header line must be properly formed and contain a non-empty, valid relative path.
        - No duplicate paths within the bundle.
        - Consider edge cases: premature EOF, missing headers, content before the first header (should it be ignored or an error?). (Implemented: content before first header is an error; other structural errors handled)
    - [x] The validation should collect _all_ format errors found in the bundle and return them as a single `anyhow::Error` (possibly by formatting a list of specific error details).
    - [x] In `src/main.rs`, call `parse_bundle`. If it returns `Err`, print the error (which should now include all validation issues) and exit. (Handled by `anyhow` and `?` operator).
    - [x] Verification:
        - Update unit tests for `parser::parse_bundle` to cover various invalid bundle file scenarios (e.g., malformed `File:` line, duplicate paths, EOF within a file block).
        - Test the `sprout` CLI with sample malformed bundle files; ensure all relevant errors are reported clearly and the program exits without attempting to write files.

## Milestone 3: File System Operations, Collision Detection, and Final Integration

**Goal:** Implement the logic to create the directory structure and files as specified in the parsed bundle. This includes robust collision detection in the output directory. This milestone will result in a fully functional `sprout` CLI for its core purpose.

- [x] **Task 3.1: Implement Output Path Collision Detection (`src/bundler.rs`)**
    
    - [x] Create the `src/bundler.rs` module.
    - [x] Implement a function, e.g., `check_for_collisions(entries: &[ParsedEntry], output_dir: &Path) -> anyhow::Result<()>`.
    - [x] For each `ParsedEntry` in the list:
        - Construct the full target path by joining `output_dir` and `entry.path`.
        - Check if this full target path already exists using `std::path::Path::exists()`.
    - [x] If any path collision is detected, this function should return an `anyhow::Error` detailing all collisions found.
    - [x] In `src/main.rs`, call this collision check function after successful bundle parsing. If it returns `Err`, print the error and exit.
    - [x] Verification:
        - Unit tests for `bundler::check_for_collisions` with scenarios: no collisions, one collision, multiple collisions, collision with a file where a directory is needed, collision with a directory where a file is needed.
        - CLI Test: `sprout` aborts with an informative error if a target file path already exists.
        - CLI Test: `sprout` aborts if a parent directory to be created (e.g., `new_dir/`) conflicts with an existing file named `new_dir`.
- [x] **Task 3.2: Implement Directory and File Creation (`src/bundler.rs`)**
    
    - [x] Implement a function, e.g., `create_files_from_bundle(entries: &[ParsedEntry], output_dir: &Path) -> anyhow::Result<()>`.
    - [x] This function is called only if bundle parsing and collision checks pass.
    - [x] For each `ParsedEntry`:
        - [x] Resolve the full absolute path for the new file.
        - [x] Ensure its parent directory exists using `std::fs::create_dir_all(parent_path)`.
        - [x] Write the `entry.content` to the file path using `std::fs::write`.
    - [x] Handle potential I/O errors during directory/file creation gracefully, returning an `anyhow::Error`.
    - [x] Verification:
        - [x] Unit tests for `bundler::create_files_from_bundle` to verify:
            - [x] Creation of a single file in the output directory.
            - [x] Creation of multiple files.
            - [x] Creation of files within newly created nested subdirectories.
            - [x] Correct writing of file content.
        - (Covered by integration tests in next task mostly)
- [x] **Task 3.3: Final Integration, User Feedback, and Testing**
    
    - [x] Integrate all components in `src/main.rs`: CLI parsing (`clap`), bundle file reading/validation (`parser.rs`), collision detection, and file/directory creation (`bundler.rs`).
    - [x] Implement clear success messages (e.g., "Successfully sprouted N files to <output_directory>.").
    - [x] Ensure all error paths (bundle format errors, I/O errors, collision errors) provide user-friendly messages propagated by `anyhow`.
    - [x] Write integration tests for the `sprout` CLI (e.g., using a test runner or simple shell scripts that invoke the compiled binary):
        - [x] Test with a valid bundle file creating a simple structure.
        - [x] Test with a valid bundle file creating a nested structure.
        - [x] Test failure with a malformed bundle file (ensure all errors are printed).
        - [x] Test failure due to output file collision (ensure specific collision is reported).
        - [x] Test with empty bundle file.
        - [x] Test output to current directory (default) and to a specified directory.
    - [x] Verification: The `sprout` command works end-to-end for valid scenarios and fails gracefully with correct, comprehensive error messages for all defined error conditions. Code coverage for core logic (parsing, bundling) is reasonable.

## Additional Tasks / Backlog

(Items from the PRD's "Future Considerations" that are out of scope for this initial prototype but good to keep in mind for future development)

- [ ] Implement Reverse Operation ("Bundling" a directory into a `digest.txt` style file).
- [x] **Implement `--force` flag for overwriting files** (2025-05-21) - Add a `--force` CLI flag to allow `sprout` to overwrite existing files in the output directory without prompting.
  - [x] Update CLI argument parsing in `src/main.rs` to include the `--force` flag.
  - [x] Modify `src/bundler.rs` to bypass collision checks and overwrite files if `--force` is active.
  - [x] Add/update unit tests for `bundler.rs` to cover overwrite logic.
  - [x] Add/update integration tests to verify `--force` flag behavior.
  - [x] Update `README.md` to document the new `--force` flag.
- [ ] Add other file overwrite protection options (skip, prompt).
- [ ] Introduce a configuration file for `sprout` (e.g., custom delimiters, default output dir).
- [ ] Add more comprehensive test cases for file system edge cases (permissions, symlinks, etc.).
- [ ] Refine and add more detailed verbose logging options (e.g., using `log` and `env_logger` crates).
- [ ] Research and implement packaging/distribution methods for the Rust binary (e.g., `cargo-dist`, GitHub Releases assets, AUR, Homebrew).
- [ ] Performance benchmarking and optimization for very large bundle files or a high number of files.
- [x] **Task 4.1: Create Project README** (2025-05-20) - Create a cool README.md with emojis, project description, usage, build, and test instructions.

## Milestone 4: Release Process & Automation

**Goal:** Define and implement an automated release process using Conventional Commits and GitHub Actions to streamline versioning, changelog generation, and GitHub Release creation.

- [x] **Task 4.2: Define and Implement Release Process using Conventional Commits** (2025-05-20)
  - [x] Research and select appropriate tooling for semantic versioning and changelog generation based on Conventional Commits (e.g., `release-please-action`).
  - [x] Create/update GitHub Actions workflow to:
    - Trigger on pushes/merges to the main branch.
    - Automatically determine the next version.
    - Generate a changelog.
    - Create a Git tag.
    - Create a GitHub Release with the changelog and release assets.
  - [x] Configure the workflow to build release binaries for common platforms (Linux, macOS, Windows).
  - [x] Document the release process and how to trigger it.
- [x] **Task 4.3: Fix GitHub Actions Workflow Permission Warnings** (2025-05-21)
  - [x] Review and address permission warnings reported for GitHub Actions workflows.
  - [x] Ensure workflows use the principle of least privilege.
  - [x] Explicitly pass `GITHUB_TOKEN` to `release-please-action` in `.github/workflows/release.yml` to address PR creation permission error. (2025-05-21)
  - [ ] Target warnings:
    - `Warn: topLevel 'security-events' permission set to 'write': .github/workflows/ci.yml:5`
    - `Warn: topLevel 'contents' permission set to 'write': .github/workflows/release.yml:9`
    - `Info: topLevel permissions set to 'read-all': .github/workflows/scorecards.yml:18`
  - [ ] Verify other 'Info' level read permissions are appropriate.


================================================
File: scripts/README.md
================================================
# Release Signing Script (`sign_releases.py`)

This Python script automates the process of cryptographically signing GitHub release artifacts for a specified repository and re-uploading them along with their `.asc` signature files. This helps meet the OpenSSF "Signed-Releases" criteria by attesting to the provenance of the artifacts.

## Prerequisites

1.  **Python 3:** Ensure you have Python 3 installed on your system.
2.  **GnuPG (GPG):** GPG must be installed, and you need to have a GPG key pair generated and configured. The script will attempt to use the first available secret key suitable for signing.
3.  **Python Libraries:** Install the necessary Python libraries using pip:
    ```bash
    pip install requests python-gnupg
    ```
4.  **GitHub Personal Access Token:** You will need a GitHub Personal Access Token.
    *   **Permissions:** The token requires the `repo` scope (or `public_repo` if your repository is public and you only need to access/modify public releases).
    *   **Usage:** The script will prompt for this token if it's not provided via the `--github-token` command-line argument or the `GITHUB_TOKEN` environment variable.

## How to Run

1.  Navigate to the `scripts` directory within your project (e.g., `cd /path/to/your/project/scripts`).
2.  Execute the script from your terminal:

    ```bash
    python sign_releases.py OWNER/REPOSITORY_NAME
    ```
    Replace `OWNER/REPOSITORY_NAME` with the target repository (e.g., `nightconcept/almandine`).

### Command-Line Arguments

*   `repo` (Required): The repository name in `owner/repo` format (e.g., `nightconcept/almandine`).
*   `--github-token YOUR_GITHUB_TOKEN` (Optional): Your GitHub Personal Access Token. If not provided, the script will try to read it from the `GITHUB_TOKEN` environment variable or prompt you to enter it.
*   `--gpg-program /path/to/gpg` (Optional): Specify the full path to your GPG executable if it's not in your system's PATH (default is `gpg`).
*   `--num-releases N` (Optional): The number of recent releases to process. Defaults to `5`. The maximum is 30 (a GitHub API limit for some queries, and the OpenSSF check looks at the 30 most recent).
*   `--skip-already-signed` (Optional): If this flag is present, the script will skip processing an asset if a corresponding `.asc` signature file already exists in the release assets.
*   `--yes` (Optional): If this flag is present, the script will automatically confirm actions (like signing and uploading) without prompting the user. Use with caution.

### Script Behavior

When executed, the script will:
1.  Prompt for your GitHub Personal Access Token if not provided via argument or environment variable.
2.  Prompt for your GPG key passphrase (if your key is passphrase-protected).
3.  Identify the first available GPG secret key suitable for signing.
4.  Fetch the specified number of recent releases from the target GitHub repository.
5.  For each release:
    a.  Iterate through its assets.
    b.  Skip any files that appear to be existing signature files (e.g., `.asc`, `.sig`).
    c.  If `--skip-already-signed` is used, skip assets that already have a corresponding `.asc` signature uploaded.
    d.  Prompt for confirmation to sign and re-upload each eligible asset (unless `--yes` is used).
    e.  Download the asset to a temporary local directory.
    f.  Sign the downloaded asset using the identified GPG key, creating a detached signature file (`.asc`).
    g.  Upload the newly created `.asc` signature file to the GitHub release.
    h.  Clean up the temporary downloaded asset and signature file.
6.  Provide logging output for all actions and any errors encountered.

## Important Considerations

*   **GPG Key Selection:** The script automatically selects the first GPG secret key it finds that is suitable for signing. Ensure the desired key is available to GPG.
*   **Idempotency:** The `--skip-already-signed` flag helps prevent re-processing assets that have already been signed and had their signatures uploaded.
*   **Error Handling:** The script includes logging and attempts to handle common errors related to GitHub API interactions, GPG operations, and file system actions.
*   **Security:**
    *   Be cautious when entering your GitHub token and GPG passphrase.
    *   Avoid hardcoding sensitive credentials directly into scripts or committing them to version control. Using environment variables or interactive prompts (as the script does) is preferred.
*   **API Rate Limits:** While the script processes releases and assets one by one, be mindful of GitHub API rate limits if you are processing a very large number of releases or assets frequently.
*   **Manual Testing:** It is highly recommended to first test this script on a fork or a test repository with a few sample releases to ensure it behaves as expected with your GPG setup and GitHub token before running it on your main project repository.


================================================
File: scripts/sign_releases.py
================================================
import os
import requests
import gnupg
import getpass
import json
import argparse
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Constants
GITHUB_API_URL = "https://api.github.com"
RELEASES_PER_PAGE = 30 # Max allowed by GitHub API for releases, check looks for 30 most recent
SIGNATURE_EXTENSIONS = [".minisig", ".asc", ".sig", ".sign", ".sigstore", ".intoto.jsonl"]

def get_github_releases(repo_owner, repo_name, token, num_releases_to_check):
    """Fetches the specified number of releases from GitHub."""
    releases = []
    page = 1
    while len(releases) < num_releases_to_check:
        url = f"{GITHUB_API_URL}/repos/{repo_owner}/{repo_name}/releases?per_page={RELEASES_PER_PAGE}&page={page}"
        headers = {"Authorization": f"token {token}"}
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            current_page_releases = response.json()
            if not current_page_releases:
                break # No more releases
            releases.extend(current_page_releases)
            if len(current_page_releases) < RELEASES_PER_PAGE:
                break # Last page
            page += 1
        except requests.exceptions.RequestException as e:
            logging.error(f"Error fetching releases: {e}")
            return None
        if len(releases) >= num_releases_to_check:
            break
    return releases[:num_releases_to_check]

def download_asset(asset_url, asset_name, token):
    """Downloads a release asset."""
    headers = {"Authorization": f"token {token}", "Accept": "application/octet-stream"}
    try:
        logging.info(f"Downloading asset: {asset_name} from {asset_url}")
        response = requests.get(asset_url, headers=headers, stream=True)
        response.raise_for_status()
        with open(asset_name, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        logging.info(f"Successfully downloaded {asset_name}")
        return asset_name
    except requests.exceptions.RequestException as e:
        logging.error(f"Error downloading asset {asset_name}: {e}")
        return None

def sign_file(gpg, filepath, keyid, passphrase):
    """Signs a file using GPG and creates a detached signature."""
    signature_file = f"{filepath}.asc"
    try:
        logging.info(f"Signing file: {filepath} with key ID {keyid}")
        with open(filepath, 'rb') as f:
            status = gpg.sign_file(f, keyid=keyid, detach=True, output=signature_file, passphrase=passphrase)

        # Check if the signing was successful.
        # The 'status' object from python-gnupg has a 'status' attribute (string)
        # and 'stderr'. Success is typically indicated by status.status == 'signature created'.
        if status and hasattr(status, 'status') and status.status == 'signature created':
            logging.info(f"Successfully signed {filepath}, signature: {signature_file}")
            return signature_file
        else:
            # Log GPG's actual status and stderr for diagnostics
            gpg_status_msg = getattr(status, 'status', 'N/A (status object might be None or lack status attribute)')
            gpg_stderr_msg = getattr(status, 'stderr', 'N/A (status object might be None or lack stderr attribute)')
            logging.error(f"Error signing file {filepath}: GPG status '{gpg_status_msg}', stderr: '{gpg_stderr_msg}'")
            if os.path.exists(signature_file): # Clean up partial signature
                os.remove(signature_file)
            return None
    except Exception as e:
        logging.error(f"Exception during signing of {filepath}: {e}")
        if os.path.exists(signature_file):
            os.remove(signature_file)
        return None

def upload_asset(upload_url_template, filepath, token):
    """Uploads an asset to a GitHub release."""
    asset_name = os.path.basename(filepath)
    # GitHub's upload_url includes path parameters like {?name,label}, remove them.
    upload_url = upload_url_template.split('{')[0] + f"?name={asset_name}"
    headers = {
        "Authorization": f"token {token}",
        "Content-Type": "application/octet-stream"
    }
    try:
        logging.info(f"Uploading asset: {asset_name} to {upload_url}")
        with open(filepath, 'rb') as f:
            response = requests.post(upload_url, headers=headers, data=f)
        response.raise_for_status()
        logging.info(f"Successfully uploaded {asset_name}")
        return response.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"Error uploading asset {asset_name}: {e}")
        if response:
            logging.error(f"Response content: {response.text}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred during upload of {asset_name}: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description="Sign GitHub release artifacts and re-upload them with signatures.")
    parser.add_argument("repo", help="Repository name in 'owner/repo' format (e.g., nightconcept/almandine).")
    parser.add_argument("--github-token", help="GitHub Personal Access Token. If not provided, will try to read from GITHUB_TOKEN env var or prompt.")
    parser.add_argument("--gpg-program", default="gpg", help="Path to GPG executable (if not in PATH).")
    parser.add_argument("--num-releases", type=int, default=5, help="Number of recent releases to process (max 30).")
    parser.add_argument("--skip-already-signed", action='store_true', help="Skip assets if a corresponding signature file already exists in the release.")
    parser.add_argument("--yes", action='store_true', help="Automatically confirm actions without prompting.")

    args = parser.parse_args()

    repo_owner, repo_name = args.repo.split('/')
    num_releases_to_check = min(args.num_releases, 30) # Cap at 30

    github_token = args.github_token or os.environ.get("GITHUB_TOKEN")
    if not github_token:
        github_token = getpass.getpass("Enter GitHub Personal Access Token: ")

    gpg = gnupg.GPG(gpgbinary=args.gpg_program)

    # Find the first available GPG secret key suitable for signing
    secret_keys = gpg.list_keys(secret=True)
    signing_key = None
    for key in secret_keys:
        for uid_details in key.get('uids', []):
            # A simple check, might need refinement based on GPG key capabilities
            if 'S' in key.get('cap', ''): # Check if key has signing capability
                signing_key = key
                break
        if signing_key:
            break

    if not signing_key:
        logging.error("No suitable GPG secret key found for signing. Please ensure you have a GPG key with signing capability.")
        logging.info("Available secret keys (if any):")
        for skey in secret_keys:
             logging.info(f"  KeyID: {skey['keyid']}, UIDs: {skey.get('uids', 'N/A')}, Capabilities: {skey.get('cap', 'N/A')}")
        return

    gpg_key_id = signing_key['keyid']
    logging.info(f"Using GPG Key ID: {gpg_key_id} ({signing_key.get('uids', ['No UID'])[0]}) for signing.")

    gpg_passphrase = getpass.getpass(f"Enter GPG passphrase for key {gpg_key_id} (leave blank if none): ")

    logging.info(f"Fetching last {num_releases_to_check} releases for {repo_owner}/{repo_name}...")
    releases = get_github_releases(repo_owner, repo_name, github_token, num_releases_to_check)

    if not releases:
        logging.info("No releases found or error fetching releases.")
        return

    for release in releases:
        release_name = release.get('name', release['tag_name'])
        logging.info(f"\nProcessing release: {release_name} (ID: {release['id']}, Tag: {release['tag_name']})")

        if 'assets' not in release or not release['assets']:
            logging.info(f"No assets found for release {release_name}.")
            continue

        upload_url_template = release['upload_url']
        existing_asset_names = {asset['name'] for asset in release['assets']}

        for asset in release['assets']:
            asset_name = asset['name']
            asset_url = asset['browser_download_url'] # This is the public URL, need API URL for download
            asset_api_url = asset['url'] # API URL for asset details and download

            # Skip if it's already a signature file
            if any(asset_name.endswith(ext) for ext in SIGNATURE_EXTENSIONS):
                logging.info(f"Skipping signature file: {asset_name}")
                continue

            # Skip if --skip-already-signed and signature exists
            signature_filename_asc = f"{asset_name}.asc"
            if args.skip_already_signed and signature_filename_asc in existing_asset_names:
                logging.info(f"Signature {signature_filename_asc} already exists for {asset_name}. Skipping.")
                continue

            if not args.yes:
                confirm = input(f"Sign and re-upload asset '{asset_name}' for release '{release_name}'? (y/N): ")
                if confirm.lower() != 'y':
                    logging.info(f"Skipping asset {asset_name} by user choice.")
                    continue

            downloaded_file_path = None
            signed_file_path = None
            temp_dir = f"temp_release_assets_{release['id']}"
            os.makedirs(temp_dir, exist_ok=True)

            original_asset_path_in_temp = os.path.join(temp_dir, asset_name)

            try:
                downloaded_file_path = download_asset(asset_api_url, original_asset_path_in_temp, github_token)
                if not downloaded_file_path:
                    continue

                signed_file_path = sign_file(gpg, downloaded_file_path, gpg_key_id, gpg_passphrase)
                if not signed_file_path:
                    continue

                # Upload original asset (if it was somehow modified or to ensure it's there)
                # This is generally not needed if we are just adding signatures,
                # but could be part of a "refresh" flow. For now, we assume original is fine.
                # If the workflow is to replace, then we'd upload downloaded_file_path.
                # For now, we only upload the signature.

                # Upload signature
                logging.info(f"Uploading signature {os.path.basename(signed_file_path)}...")
                upload_asset(upload_url_template, signed_file_path, github_token)

            finally:
                # Clean up temporary files
                if downloaded_file_path and os.path.exists(downloaded_file_path):
                    os.remove(downloaded_file_path)
                if signed_file_path and os.path.exists(signed_file_path):
                    os.remove(signed_file_path)
                if os.path.exists(temp_dir) and not os.listdir(temp_dir): # Remove dir if empty
                    os.rmdir(temp_dir)
                elif os.path.exists(temp_dir) and os.listdir(temp_dir):
                    logging.warning(f"Temporary directory {temp_dir} is not empty after processing asset {asset_name}. Manual cleanup may be required.")


    logging.info("\nScript finished.")

if __name__ == "__main__":
    main()



================================================
File: src/bundler.rs
================================================
// src/bundler.rs
// Module for file/directory creation and output logic

use crate::parser::ParsedEntry;
use anyhow::{Context, Result};
use std::{
    fs,
    path::{Path, PathBuf},
};

/// Creates directories and files based on the parsed bundle entries.
///
/// This function is called only if bundle parsing and collision checks pass.
/// For each `ParsedEntry`:
///   - Resolves the full absolute path for the new file.
///   - Ensures its parent directory exists using `std::fs::create_dir_all(parent_path)`.
///   - Writes the `entry.content` to the file path using `std::fs::write`.
///
/// Handles potential I/O errors during directory/file creation gracefully, returning an `anyhow::Error`.
/// If `force` is true, existing files will be overwritten.
pub fn create_files_from_bundle(
    entries: &[ParsedEntry],
    output_dir: &Path,
    _force: bool, // Indicate unused variable, logic is handled by skipping collision check
) -> Result<()> {
    for entry in entries {
        let full_target_path = output_dir.join(&entry.path);

        // If forcing, we don't care if the file exists, but we still need to ensure parent dirs are there.
        // If not forcing, collision check should have already happened.
        if let Some(parent_path) = full_target_path.parent() {
            if !parent_path.exists() {
                fs::create_dir_all(parent_path).with_context(|| {
                    format!("Failed to create parent directory: {:?}", parent_path)
                })?;
            } else if parent_path.is_file() {
                // This case should ideally be caught by check_for_collisions if not forcing.
                // If forcing, and a parent path component is a file, fs::write will fail later.
                // This is a safeguard or clarity, fs::write would fail anyway.
                return Err(anyhow::anyhow!(
                    "Cannot create file {:?}, its parent {:?} is an existing file.",
                    full_target_path,
                    parent_path
                ));
            }
        }

        // fs::write will overwrite if the path exists and is a file.
        // If path is a directory, fs::write will fail, which is correct.
        fs::write(&full_target_path, &entry.content)
            .with_context(|| format!("Failed to write file: {:?}", full_target_path))?;
    }
    Ok(())
}

/// Checks for path collisions in the output directory.
///
/// For each `ParsedEntry`, it constructs the full target path by joining
/// `output_dir` and `entry.path`. It then checks if this full target path
/// already exists. If any collisions are detected, it returns an `anyhow::Error`
/// detailing all collisions.
pub fn check_for_collisions(entries: &[ParsedEntry], output_dir: &Path) -> Result<()> {
    let mut collisions = Vec::new();

    for entry in entries {
        let target_path = output_dir.join(&entry.path);
        if target_path.exists() {
            collisions.push(target_path);
        } else {
            let mut current_check_path = PathBuf::new();
            for component in entry
                .path
                .parent()
                .unwrap_or_else(|| Path::new(""))
                .components()
            {
                current_check_path.push(component);
                let full_component_path = output_dir.join(&current_check_path);
                if full_component_path.is_file()
                    && entry
                        .path
                        .strip_prefix(&current_check_path)
                        .is_ok_and(|p| !p.as_os_str().is_empty())
                {
                    collisions.push(full_component_path);
                    break;
                }
            }
        }
    }

    if !collisions.is_empty() {
        let collision_details = collisions
            .iter()
            .map(|p| format!("  - {}", p.display()))
            .collect::<Vec<String>>()
            .join("\n");
        return Err(anyhow::anyhow!(
            "Output path collision detected. The following paths already exist or conflict with directory creation:\n{}",
            collision_details
        ));
    }

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::parser::ParsedEntry;
    use std::fs::{self, File};
    use tempfile::tempdir;

    fn create_parsed_entry(path_str: &str, content_str: &str) -> ParsedEntry {
        ParsedEntry {
            path: PathBuf::from(path_str),
            content: String::from(content_str),
        }
    }

    #[test]
    fn test_check_for_collisions_no_collision() {
        let dir = tempdir().unwrap();
        let output_dir = dir.path();
        let entries = vec![
            create_parsed_entry("file1.txt", "content1"),
            create_parsed_entry("dir1/file2.txt", "content2"),
        ];

        let result = check_for_collisions(&entries, output_dir);
        assert!(result.is_ok());
    }

    #[test]
    fn test_check_for_collisions_single_file_collision() {
        let dir = tempdir().unwrap();
        let output_dir = dir.path();
        File::create(output_dir.join("file1.txt")).unwrap();

        let entries = vec![
            create_parsed_entry("file1.txt", "content1"),
            create_parsed_entry("file2.txt", "content2"),
        ];

        let result = check_for_collisions(&entries, output_dir);
        assert!(result.is_err());
        let error_message = result.err().unwrap().to_string();
        assert!(error_message.contains("Output path collision detected"));
        assert!(error_message.contains(&output_dir.join("file1.txt").display().to_string()));
    }

    #[test]
    fn test_check_for_collisions_multiple_file_collisions() {
        let dir = tempdir().unwrap();
        let output_dir = dir.path();
        File::create(output_dir.join("file1.txt")).unwrap();
        fs::create_dir_all(output_dir.join("dir1")).unwrap();
        File::create(output_dir.join("dir1/file2.txt")).unwrap();

        let entries = vec![
            create_parsed_entry("file1.txt", "c1"),
            create_parsed_entry("dir1/file2.txt", "c2"),
            create_parsed_entry("file3.txt", "c3"),
        ];

        let result = check_for_collisions(&entries, output_dir);
        assert!(result.is_err());
        let error_message = result.err().unwrap().to_string();
        assert!(error_message.contains(&output_dir.join("file1.txt").display().to_string()));
        assert!(error_message.contains(&output_dir.join("dir1/file2.txt").display().to_string()));
    }

    #[test]
    fn test_check_for_collisions_directory_as_file_collision() {
        let dir = tempdir().unwrap();
        let output_dir = dir.path();
        fs::create_dir_all(output_dir.join("item")).unwrap();

        let entries = vec![create_parsed_entry("item", "content")];

        let result = check_for_collisions(&entries, output_dir);
        assert!(result.is_err());
        let error_message = result.err().unwrap().to_string();
        assert!(error_message.contains(&output_dir.join("item").display().to_string()));
    }

    #[test]
    fn test_check_for_collisions_file_as_directory_collision() {
        let dir = tempdir().unwrap();
        let output_dir = dir.path();
        File::create(output_dir.join("item")).unwrap();

        let entries = vec![create_parsed_entry("item/another.txt", "content")];

        let result = check_for_collisions(&entries, output_dir);
        assert!(result.is_err());
        let error_message = result.err().unwrap().to_string();
        assert!(error_message.contains(&output_dir.join("item").display().to_string()));
        assert!(error_message.contains("conflict with directory creation"));
    }

    #[test]
    fn test_check_for_collisions_deep_file_as_directory_collision() {
        let dir = tempdir().unwrap();
        let output_dir = dir.path();
        fs::create_dir_all(output_dir.join("level1")).unwrap();
        File::create(output_dir.join("level1/item")).unwrap();

        let entries = vec![create_parsed_entry("level1/item/another.txt", "content")];

        let result = check_for_collisions(&entries, output_dir);
        assert!(result.is_err());
        let error_message = result.err().unwrap().to_string();
        assert!(error_message.contains(&output_dir.join("level1/item").display().to_string()));
    }

    #[test]
    fn test_create_single_file() -> Result<()> {
        let dir = tempdir()?;
        let output_dir = dir.path();
        let entries = vec![create_parsed_entry("file1.txt", "Hello World")];

        create_files_from_bundle(&entries, output_dir, false)?;

        let file_path = output_dir.join("file1.txt");
        assert!(file_path.exists());
        assert_eq!(fs::read_to_string(file_path)?, "Hello World");
        Ok(())
    }

    #[test]
    fn test_create_multiple_files() -> Result<()> {
        let dir = tempdir()?;
        let output_dir = dir.path();
        let entries = vec![
            create_parsed_entry("file1.txt", "Content 1"),
            create_parsed_entry("file2.txt", "Content 2"),
        ];

        create_files_from_bundle(&entries, output_dir, false)?;

        let file_path1 = output_dir.join("file1.txt");
        assert!(file_path1.exists());
        assert_eq!(fs::read_to_string(file_path1)?, "Content 1");

        let file_path2 = output_dir.join("file2.txt");
        assert!(file_path2.exists());
        assert_eq!(fs::read_to_string(file_path2)?, "Content 2");
        Ok(())
    }

    #[test]
    fn test_create_files_in_nested_directories() -> Result<()> {
        let dir = tempdir()?;
        let output_dir = dir.path();
        let entries = vec![
            create_parsed_entry("dir1/file1.txt", "Nested Content 1"),
            create_parsed_entry("dir1/dir2/file2.txt", "Deeply Nested Content 2"),
            create_parsed_entry("file3.txt", "Root Content 3"),
        ];

        create_files_from_bundle(&entries, output_dir, false)?;

        let path1 = output_dir.join("dir1/file1.txt");
        assert!(path1.exists());
        assert_eq!(fs::read_to_string(path1)?, "Nested Content 1");
        assert!(output_dir.join("dir1").is_dir());

        let path2 = output_dir.join("dir1/dir2/file2.txt");
        assert!(path2.exists());
        assert_eq!(fs::read_to_string(path2)?, "Deeply Nested Content 2");
        assert!(output_dir.join("dir1/dir2").is_dir());

        let path3 = output_dir.join("file3.txt");
        assert!(path3.exists());
        assert_eq!(fs::read_to_string(path3)?, "Root Content 3");
        Ok(())
    }

    #[test]
    fn test_create_file_with_empty_content() -> Result<()> {
        let dir = tempdir()?;
        let output_dir = dir.path();
        let entries = vec![create_parsed_entry("empty.txt", "")];

        create_files_from_bundle(&entries, output_dir, false)?;

        let file_path = output_dir.join("empty.txt");
        assert!(file_path.exists());
        assert_eq!(fs::read_to_string(file_path)?, "");
        Ok(())
    }

    #[test]
    fn test_create_files_complex_paths_and_content() -> Result<()> {
        let dir = tempdir()?;
        let output_dir = dir.path();
        let entries = vec![
            create_parsed_entry("src/main.rs", "fn main() {\n    println!(\"Hello\");\n}"),
            create_parsed_entry("docs/README.md", "# My Project\n\nThis is a test."),
            create_parsed_entry("config/settings.toml", "key = \"value\"\nnumber = 123"),
        ];

        create_files_from_bundle(&entries, output_dir, false)?;

        let path_rs = output_dir.join("src/main.rs");
        assert!(path_rs.exists());
        assert_eq!(
            fs::read_to_string(path_rs)?,
            "fn main() {\n    println!(\"Hello\");\n}"
        );
        assert!(output_dir.join("src").is_dir());

        let path_md = output_dir.join("docs/README.md");
        assert!(path_md.exists());
        assert_eq!(
            fs::read_to_string(path_md)?,
            "# My Project\n\nThis is a test."
        );
        assert!(output_dir.join("docs").is_dir());

        let path_toml = output_dir.join("config/settings.toml");
        assert!(path_toml.exists());
        assert_eq!(
            fs::read_to_string(path_toml)?,
            "key = \"value\"\nnumber = 123"
        );
        assert!(output_dir.join("config").is_dir());

        Ok(())
    }

    #[test]
    fn test_create_files_overwrite_with_force() -> Result<()> {
        let dir = tempdir()?;
        let output_dir = dir.path();
        let file_path = output_dir.join("file1.txt");

        // Create an initial file
        fs::write(&file_path, "Initial Content")?;
        assert_eq!(fs::read_to_string(&file_path)?, "Initial Content");

        let entries = vec![create_parsed_entry("file1.txt", "Overwritten Content")];

        // Create files with force=true
        create_files_from_bundle(&entries, output_dir, true)?;

        assert!(file_path.exists());
        assert_eq!(fs::read_to_string(&file_path)?, "Overwritten Content");
        Ok(())
    }

    #[test]
    fn test_create_files_fail_on_parent_is_file_even_with_force() -> Result<()> {
        let dir = tempdir()?;
        let output_dir = dir.path();
        let file_acting_as_parent_path = output_dir.join("parent_file");

        // Create a file where a directory is expected
        fs::write(&file_acting_as_parent_path, "I am a file, not a directory.")?;

        let entries = vec![create_parsed_entry(
            "parent_file/child.txt",
            "This should not be written.",
        )];

        // Attempt to create files with force=true
        let result = create_files_from_bundle(&entries, output_dir, true);

        assert!(result.is_err());
        let error_message = result.err().unwrap().to_string();
        assert!(error_message.contains("its parent"));
        assert!(error_message.contains("is an existing file"));

        // Ensure the original "parent_file" is untouched and no "child.txt" was created
        assert_eq!(
            fs::read_to_string(&file_acting_as_parent_path)?,
            "I am a file, not a directory."
        );
        assert!(!output_dir.join("parent_file/child.txt").exists());

        Ok(())
    }
}



================================================
File: src/main.rs
================================================
use clap::Parser;
use std::path::PathBuf;

mod bundler;
mod parser;

/// sprout - A CLI tool to sprout files from a bundle.
#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None, infer_long_args = true)]
struct CliArgs {
    /// Path to the bundle file (positional).
    /// Required unless -i/--input is used.
    #[arg(name = "BUNDLE_FILE_PATH", required_unless_present = "input")]
    bundle_file_path: Option<PathBuf>,

    /// Output directory path (positional).
    /// Defaults to the current directory if not specified and -o/--output is not used.
    #[arg(name = "OUTPUT_DIRECTORY_PATH", default_value = ".")]
    output_directory_path: PathBuf,

    /// Specify bundle file path via flag (alternative to positional BUNDLE_FILE_PATH).
    #[arg(short, long, value_name = "PATH", conflicts_with = "BUNDLE_FILE_PATH")]
    input: Option<PathBuf>,

    /// Specify output directory path via flag (overrides positional OUTPUT_DIRECTORY_PATH).
    #[arg(short, long, value_name = "PATH")]
    output: Option<PathBuf>,

    /// Force overwrite of existing files.
    #[arg(short, long, default_value_t = false)]
    force: bool,
}

fn main() -> anyhow::Result<()> {
    let args = CliArgs::parse();

    let bundle_path = match (args.bundle_file_path, args.input) {
        (Some(p), None) => p,
        (None, Some(i)) => i,
        _ => unreachable!("Clap should ensure one input source is exclusively provided and valid."),
    };

    let final_output_path = if let Some(output_flag_path) = args.output {
        output_flag_path
    } else {
        args.output_directory_path
    };

    let parsed_data = parser::parse_bundle(&bundle_path)?;

    if parsed_data.is_empty() {
        println!(
            "Bundle file '{}' is empty or contains no valid entries. Nothing to sprout.",
            bundle_path.display()
        );
        return Ok(());
    }

    if !args.force {
        bundler::check_for_collisions(&parsed_data, &final_output_path)?;
    }

    bundler::create_files_from_bundle(&parsed_data, &final_output_path, args.force)?;

    println!(
        "Successfully sprouted {} file(s) from '{}' to '{}'.{}",
        parsed_data.len(),
        bundle_path.display(),
        final_output_path.display(),
        if args.force {
            " (files overwritten if necessary)"
        } else {
            ""
        }
    );
    Ok(())
}



================================================
File: src/parser.rs
================================================
// src/parser.rs
// Module for parsing the bundle file

use anyhow::{Context, Result, anyhow};
use std::collections::HashSet;
use std::fmt;
use std::fs;
use std::path::{Path, PathBuf};

const FILE_HEADER_SEPARATOR: &str = "================================================";
const FILE_PATH_PREFIX: &str = "File: ";

/// Represents a single parsed file entry from the bundle.
#[derive(Debug, PartialEq, Eq, Clone)]
pub struct ParsedEntry {
    pub path: PathBuf,
    pub content: String,
}

/// Specific errors that can occur during bundle parsing and validation.
#[derive(Debug, PartialEq, Eq, Clone)]
pub enum BundleValidationError {
    ContentBeforeFirstHeader {
        line_number: usize,
        content_excerpt: String,
    },
    MalformedHeaderMissingFilePrefix {
        line_number: usize,
        header_line: String,
    },
    MalformedHeaderMissingSeparatorAfterPath {
        line_number: usize,
        path_line: String,
    },
    MalformedHeaderPathLineInterruptedBySeparator {
        line_number: usize,
        path_line: String,
    },
    MalformedHeaderPathLineMissingNewline {
        line_number: usize,
        path_line: String,
    },
    MalformedHeaderMissingNewlineAfterContentSeparator {
        line_number: usize,
        separator_line: String,
    },
    EmptyPath {
        line_number: usize,
    },
    AbsolutePathNotAllowed {
        line_number: usize,
        path: String,
    },
    DuplicatePath {
        line_number: usize,
        path: String,
    },
    PrematureEOFBeforePathLine {
        line_number: usize,
    },
    PrematureEOFBeforeContentSeparator {
        line_number: usize,
        path: String,
    },
    PrematureEOFBeforeContentSeparatorNewline {
        line_number: usize,
        path: String,
    },
    UnexpectedContentAfterLastEntry {
        line_number: usize,
        content_excerpt: String,
    },
}

impl fmt::Display for BundleValidationError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            BundleValidationError::ContentBeforeFirstHeader {
                line_number,
                content_excerpt,
            } => write!(
                f,
                "L{}: Content found before the first file header. Starts with: \"{}\"",
                line_number, content_excerpt
            ),
            BundleValidationError::MalformedHeaderMissingFilePrefix {
                line_number,
                header_line,
            } => write!(
                f,
                "L{}: Malformed file header. Expected '{}' after separator line, found: \"{}\"",
                line_number, FILE_PATH_PREFIX, header_line
            ),
            BundleValidationError::MalformedHeaderMissingSeparatorAfterPath {
                line_number,
                path_line,
            } => write!(
                f,
                "L{}: Malformed file header. Expected separator line after path line, found: \"{}\"",
                line_number, path_line
            ),
            BundleValidationError::MalformedHeaderPathLineInterruptedBySeparator {
                line_number,
                path_line,
            } => write!(
                f,
                "L{}: Malformed file header. File path line is interrupted by a separator: \"{}\"",
                line_number, path_line
            ),
            BundleValidationError::MalformedHeaderPathLineMissingNewline {
                line_number,
                path_line,
            } => write!(
                f,
                "L{}: Malformed file header. File path line does not end with a newline: \"{}\"",
                line_number, path_line
            ),
            BundleValidationError::MalformedHeaderMissingNewlineAfterContentSeparator {
                line_number,
                separator_line,
            } => write!(
                f,
                "L{}: Malformed file header. Expected newline after content separator line: \"{}\"",
                line_number, separator_line
            ),
            BundleValidationError::EmptyPath { line_number } => {
                write!(f, "L{}: File path is empty.", line_number)
            }
            BundleValidationError::AbsolutePathNotAllowed { line_number, path } => write!(
                f,
                "L{}: Absolute path not allowed: \"{}\"",
                line_number, path
            ),
            BundleValidationError::DuplicatePath { line_number, path } => {
                write!(f, "L{}: Duplicate path found: \"{}\"", line_number, path)
            }
            BundleValidationError::PrematureEOFBeforePathLine { line_number } => write!(
                f,
                "L{}: Premature EOF. Expected 'File: <path>' line after separator.",
                line_number
            ),
            BundleValidationError::PrematureEOFBeforeContentSeparator { line_number, path } => {
                write!(
                    f,
                    "L{}: Premature EOF for file \"{}\". Expected second separator line after path.",
                    path, line_number
                )
            }
            BundleValidationError::PrematureEOFBeforeContentSeparatorNewline {
                line_number,
                path,
            } => write!(
                f,
                "L{}: Premature EOF for file \"{}\". Expected newline after content separator.",
                path, line_number
            ),
            BundleValidationError::UnexpectedContentAfterLastEntry {
                line_number,
                content_excerpt,
            } => write!(
                f,
                "L{}: Unexpected content found after the last valid file entry. Starts with: \"{}\"",
                line_number, content_excerpt
            ),
        }
    }
}

/// Container for multiple validation errors.
#[derive(Debug)]
pub struct BundleParseError {
    pub errors: Vec<BundleValidationError>,
}

impl fmt::Display for BundleParseError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        writeln!(
            f,
            "Bundle parsing failed with {} error(s):",
            self.errors.len()
        )?;
        for error in &self.errors {
            writeln!(f, "- {}", error)?;
        }
        Ok(())
    }
}

impl std::error::Error for BundleParseError {}

/// Parses a bundle file, extracting file paths and their content, and validating the format.
///
/// Collects all format errors found in the bundle.
pub fn parse_bundle(bundle_path: &Path) -> Result<Vec<ParsedEntry>> {
    let bundle_content = fs::read_to_string(bundle_path)
        .with_context(|| format!("Failed to read bundle file: {:?}", bundle_path))?;

    if bundle_content.trim().is_empty() {
        return Ok(Vec::new());
    }

    let mut entries = Vec::new();
    let mut validation_errors = Vec::new();
    let mut paths_seen = HashSet::new();

    let lines: Vec<&str> = bundle_content.lines().collect();

    let mut first_header_line_idx: Option<usize> = None;
    for (idx, line_content) in lines.iter().enumerate() {
        if line_content.trim_start().starts_with(FILE_HEADER_SEPARATOR)
            && idx + 1 < lines.len()
            && lines[idx + 1].trim_start().starts_with(FILE_PATH_PREFIX)
        {
            first_header_line_idx = Some(idx);
            break;
        }
    }

    let mut start_processing_from_line_idx = 0;
    let mut skipped_pre_header_line_numbers: Vec<usize> = Vec::new();

    if let Some(fh_idx) = first_header_line_idx {
        for (line_idx, line_content) in lines.iter().enumerate().take(fh_idx) {
            if !line_content.trim().is_empty() {
                skipped_pre_header_line_numbers.push(line_idx + 1);
            }
        }

        if !skipped_pre_header_line_numbers.is_empty() {
            let min_line = *skipped_pre_header_line_numbers.iter().min().unwrap();
            let max_line = *skipped_pre_header_line_numbers.iter().max().unwrap();
            if min_line == max_line {
                eprintln!(
                    "Warning: Line {} excluded due to content before the first file header.",
                    min_line
                );
            } else {
                eprintln!(
                    "Warning: Lines {}-{} excluded due to content before the first file header.",
                    min_line, max_line
                );
            }
        }
        start_processing_from_line_idx = fh_idx;
    } else if !bundle_content.trim().is_empty() {
        let first_actual_content_line_str = lines
            .iter()
            .find(|line| !line.trim().is_empty())
            .map_or("", |line| line.trim());

        validation_errors.push(BundleValidationError::ContentBeforeFirstHeader {
            line_number: 1,
            content_excerpt: first_actual_content_line_str.chars().take(50).collect(),
        });
    }

    let mut current_bundle_offset = 0;
    for line_content_str in lines.iter().take(start_processing_from_line_idx) {
        current_bundle_offset += line_content_str.len() + 1;
    }

    while current_bundle_offset < bundle_content.len() {
        let remaining_content = &bundle_content[current_bundle_offset..];
        let search_start_line = bundle_content[..current_bundle_offset].lines().count();

        match remaining_content.find(FILE_HEADER_SEPARATOR) {
            Some(header_relative_start) => {
                let header_absolute_start = current_bundle_offset + header_relative_start;
                let header_line_number =
                    bundle_content[..header_absolute_start].lines().count() + 1;

                let skipped_content = &bundle_content[current_bundle_offset..header_absolute_start];
                if !skipped_content.trim().is_empty() {
                    validation_errors.push(
                        BundleValidationError::UnexpectedContentAfterLastEntry {
                            line_number: search_start_line,
                            content_excerpt: skipped_content
                                .trim()
                                .lines()
                                .next()
                                .unwrap_or("")
                                .chars()
                                .take(50)
                                .collect(),
                        },
                    );
                }

                let current_separator_line_num = header_line_number;

                let after_first_sep_start = header_absolute_start + FILE_HEADER_SEPARATOR.len();
                if after_first_sep_start >= bundle_content.len() {
                    validation_errors.push(BundleValidationError::PrematureEOFBeforePathLine {
                        line_number: current_separator_line_num,
                    });
                    current_bundle_offset = bundle_content.len();
                    continue;
                }
                if bundle_content.as_bytes()[after_first_sep_start] != b'\n' {
                    validation_errors.push(
                        BundleValidationError::MalformedHeaderMissingFilePrefix {
                            line_number: current_separator_line_num + 1,
                            header_line: bundle_content[after_first_sep_start..]
                                .lines()
                                .next()
                                .unwrap_or("")
                                .trim_end()
                                .to_string(),
                        },
                    );
                    current_bundle_offset = bundle_content.len();
                    continue;
                }
                let path_line_num = current_separator_line_num + 1;

                let path_line_start = after_first_sep_start + 1;
                if path_line_start >= bundle_content.len() {
                    validation_errors.push(BundleValidationError::PrematureEOFBeforePathLine {
                        line_number: path_line_num,
                    });
                    current_bundle_offset = bundle_content.len();
                    continue;
                }
                if !bundle_content[path_line_start..].starts_with(FILE_PATH_PREFIX) {
                    validation_errors.push(
                        BundleValidationError::MalformedHeaderMissingFilePrefix {
                            line_number: path_line_num,
                            header_line: bundle_content[path_line_start..]
                                .lines()
                                .next()
                                .unwrap_or("")
                                .to_string(),
                        },
                    );
                    current_bundle_offset = bundle_content.len();
                    continue;
                }

                let path_actual_start = path_line_start + FILE_PATH_PREFIX.len();
                let path_line_terminator_search_slice = &bundle_content[path_actual_start..];
                let newline_pos_in_slice = path_line_terminator_search_slice.find('\n');

                let path_str_end_offset = match newline_pos_in_slice {
                    Some(nl_idx) => {
                        if path_line_terminator_search_slice[..nl_idx]
                            .contains(FILE_HEADER_SEPARATOR)
                        {
                            validation_errors.push(BundleValidationError::MalformedHeaderPathLineInterruptedBySeparator {
                                line_number: path_line_num,
                                path_line: bundle_content[path_actual_start .. path_actual_start + nl_idx].trim_end().to_string(),
                            });
                            current_bundle_offset = bundle_content.len();
                            continue;
                        }
                        path_actual_start + nl_idx
                    }
                    None => {
                        validation_errors.push(
                            BundleValidationError::MalformedHeaderPathLineMissingNewline {
                                line_number: path_line_num,
                                path_line: path_line_terminator_search_slice
                                    .lines()
                                    .next()
                                    .unwrap_or("")
                                    .trim_end()
                                    .to_string(),
                            },
                        );
                        current_bundle_offset = bundle_content.len();
                        continue;
                    }
                };

                let file_path_str = bundle_content[path_actual_start..path_str_end_offset].trim();
                if file_path_str.is_empty() {
                    validation_errors.push(BundleValidationError::EmptyPath {
                        line_number: path_line_num,
                    });
                }

                let path = PathBuf::from(file_path_str);
                if path.is_absolute() {
                    validation_errors.push(BundleValidationError::AbsolutePathNotAllowed {
                        line_number: path_line_num,
                        path: file_path_str.to_string(),
                    });
                }
                if !file_path_str.is_empty()
                    && !path.is_absolute()
                    && !paths_seen.insert(path.clone())
                {
                    validation_errors.push(BundleValidationError::DuplicatePath {
                        line_number: path_line_num,
                        path: file_path_str.to_string(),
                    });
                }
                let second_sep_line_num = path_line_num + 1;

                let second_sep_start = path_str_end_offset + 1;
                if second_sep_start >= bundle_content.len() {
                    validation_errors.push(
                        BundleValidationError::PrematureEOFBeforeContentSeparator {
                            line_number: second_sep_line_num,
                            path: file_path_str.to_string(),
                        },
                    );
                    current_bundle_offset = bundle_content.len();
                    continue;
                }
                if !bundle_content[second_sep_start..].starts_with(FILE_HEADER_SEPARATOR) {
                    validation_errors.push(
                        BundleValidationError::MalformedHeaderMissingSeparatorAfterPath {
                            line_number: second_sep_line_num,
                            path_line: file_path_str.to_string(),
                        },
                    );
                    current_bundle_offset = bundle_content.len();
                    continue;
                }

                let after_second_sep_start = second_sep_start + FILE_HEADER_SEPARATOR.len();
                if after_second_sep_start >= bundle_content.len() {
                    validation_errors.push(
                        BundleValidationError::PrematureEOFBeforeContentSeparatorNewline {
                            line_number: second_sep_line_num,
                            path: file_path_str.to_string(),
                        },
                    );
                    current_bundle_offset = bundle_content.len();
                    continue;
                }
                if bundle_content.as_bytes()[after_second_sep_start] != b'\n' {
                    validation_errors.push(
                        BundleValidationError::MalformedHeaderMissingNewlineAfterContentSeparator {
                            line_number: second_sep_line_num,
                            separator_line: bundle_content[second_sep_start
                                ..std::cmp::min(
                                    bundle_content.len(),
                                    second_sep_start + FILE_HEADER_SEPARATOR.len(),
                                )]
                                .trim_end()
                                .to_string(),
                        },
                    );
                    current_bundle_offset = bundle_content.len();
                    continue;
                }

                let content_actual_start = after_second_sep_start + 1;

                let next_entry_header_search_start = content_actual_start;
                let content_end_offset = bundle_content[next_entry_header_search_start..]
                    .find(FILE_HEADER_SEPARATOR)
                    .map(|pos| next_entry_header_search_start + pos)
                    .unwrap_or_else(|| bundle_content.len());

                let content = bundle_content[content_actual_start..content_end_offset].to_string();

                if !file_path_str.is_empty() && !path.is_absolute() {
                    entries.push(ParsedEntry { path, content });
                }

                current_bundle_offset = content_end_offset;
            }
            None => {
                let final_remaining_content = &bundle_content[current_bundle_offset..];
                if !final_remaining_content.trim().is_empty() && !entries.is_empty() {
                    validation_errors.push(
                        BundleValidationError::UnexpectedContentAfterLastEntry {
                            line_number: bundle_content[..current_bundle_offset].lines().count()
                                + 1,
                            content_excerpt: final_remaining_content
                                .trim()
                                .lines()
                                .next()
                                .unwrap_or("")
                                .chars()
                                .take(50)
                                .collect(),
                        },
                    );
                }
                current_bundle_offset = bundle_content.len();
            }
        }
    }

    if !validation_errors.is_empty() {
        return Err(anyhow!(BundleParseError {
            errors: validation_errors
        }));
    }

    Ok(entries)
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::io::Write;
    use tempfile::NamedTempFile;

    fn create_temp_bundle_file(content: &str) -> NamedTempFile {
        let mut temp_file = NamedTempFile::new().expect("Failed to create temp file");
        write!(temp_file, "{}", content).expect("Failed to write to temp file");
        temp_file
    }

    fn assert_specific_error(
        result: &Result<Vec<ParsedEntry>, anyhow::Error>,
        expected_error: BundleValidationError,
    ) {
        match result {
            Err(err) => {
                if let Some(bundle_parse_error) = err.downcast_ref::<BundleParseError>() {
                    assert!(
                        bundle_parse_error.errors.contains(&expected_error),
                        "Expected error {:?} not found in {:?}",
                        expected_error,
                        bundle_parse_error.errors
                    );
                } else {
                    panic!("Error is not a BundleParseError: {:?}", err);
                }
            }
            Ok(_) => panic!("Expected error, but got Ok"),
        }
    }

    #[test]
    fn test_parse_empty_bundle_file() {
        let temp_file = create_temp_bundle_file("");
        let entries = parse_bundle(temp_file.path()).unwrap();
        assert!(entries.is_empty());
    }

    #[test]
    fn test_parse_bundle_file_with_only_whitespace() {
        let temp_file = create_temp_bundle_file("   \n\t  \n");
        let entries = parse_bundle(temp_file.path()).unwrap();
        assert!(entries.is_empty());
    }

    #[test]
    fn test_error_content_before_first_header() {
        let bundle_content = format!(
            "Some introductory text.\n\
            {}\n\
            {}path/to/file1.txt\n\
            {}\n\
            Content of file1.",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX, FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert!(result.is_ok(), "Expected Ok, got {:?}", result);
        let entries = result.unwrap();
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].path, PathBuf::from("path/to/file1.txt"));
        assert_eq!(entries[0].content, "Content of file1.");
    }

    #[test]
    fn test_error_content_before_first_header_no_valid_header_at_all() {
        let temp_file =
            create_temp_bundle_file("This is just some text, no valid file entries at all.");
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::ContentBeforeFirstHeader {
                line_number: 1,
                content_excerpt: "This is just some text, no valid file entries at a".to_string(),
            },
        );
    }

    #[test]
    fn test_parse_single_entry() {
        let bundle_content = format!(
            "{}\n\
            {}file.txt\n\
            {}\n\
            Hello, world!",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX, FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let entries = parse_bundle(temp_file.path()).unwrap();
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].path, PathBuf::from("file.txt"));
        assert_eq!(entries[0].content, "Hello, world!");
    }

    #[test]
    fn test_parse_multiple_entries() {
        let bundle_content = format!(
            "{}\n\
            {}file1.txt\n\
            {}\n\
            Content of file1.\n\
            {}\n\
            {}path/to/file2.rs\n\
            {}\n\
            // Rust code\nfn main() {{}}\n\
            {}\n\
            {}another.md\n\
            {}\n\
            ## Markdown Content",
            FILE_HEADER_SEPARATOR,
            FILE_PATH_PREFIX,
            FILE_HEADER_SEPARATOR,
            FILE_HEADER_SEPARATOR,
            FILE_PATH_PREFIX,
            FILE_HEADER_SEPARATOR,
            FILE_HEADER_SEPARATOR,
            FILE_PATH_PREFIX,
            FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let entries = parse_bundle(temp_file.path()).unwrap();
        assert_eq!(entries.len(), 3);

        assert_eq!(entries[0].path, PathBuf::from("file1.txt"));
        assert_eq!(entries[0].content, "Content of file1.\n");

        assert_eq!(entries[1].path, PathBuf::from("path/to/file2.rs"));
        assert_eq!(entries[1].content, "// Rust code\nfn main() {}\n");

        assert_eq!(entries[2].path, PathBuf::from("another.md"));
        assert_eq!(entries[2].content, "## Markdown Content");
    }

    #[test]
    fn test_parse_entry_with_empty_content() {
        let bundle_content = format!(
            "{}\n\
            {}empty_file.txt\n\
            {}\n",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX, FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let entries = parse_bundle(temp_file.path()).unwrap();
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].path, PathBuf::from("empty_file.txt"));
        assert_eq!(entries[0].content, "");
    }

    #[test]
    fn test_error_malformed_header_missing_file_prefix() {
        let bundle_content = format!(
            "{}\n\
            Not File: path/to/file.txt\n\
            {}\n\
            Content",
            FILE_HEADER_SEPARATOR, FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::MalformedHeaderMissingFilePrefix {
                line_number: 2,
                header_line: "Not File: path/to/file.txt".to_string(),
            },
        );
    }

    #[test]
    fn test_error_malformed_header_missing_separator_after_path() {
        let bundle_content = format!(
            "{}\n\
            {}path/to/file.txt\n\
            Content without second separator",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::MalformedHeaderMissingSeparatorAfterPath {
                line_number: 3,
                path_line: "path/to/file.txt".to_string(),
            },
        );
    }

    #[test]
    fn test_error_path_line_interrupted_by_separator() {
        let bundle_content = format!(
            "{}\n\
            {}path/to{}file.txt\n\
            {}\n\
            Content",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX, FILE_HEADER_SEPARATOR, FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::MalformedHeaderPathLineInterruptedBySeparator {
                line_number: 2,
                path_line: format!("path/to{}file.txt", FILE_HEADER_SEPARATOR),
            },
        );
    }

    #[test]
    fn test_error_path_line_missing_newline() {
        let bundle_content = format!(
            "{}\n\
            {}path/to/file.txt",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::MalformedHeaderPathLineMissingNewline {
                line_number: 2,
                path_line: "path/to/file.txt".to_string(),
            },
        );
    }

    #[test]
    fn test_error_missing_newline_after_content_separator() {
        let bundle_content = format!(
            "{}\n\
            {}file.txt\n\
            {}{}",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX, FILE_HEADER_SEPARATOR, "NoNewlineContent"
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::MalformedHeaderMissingNewlineAfterContentSeparator {
                line_number: 3,
                separator_line: FILE_HEADER_SEPARATOR.to_string(),
            },
        );
    }

    #[test]
    fn test_error_empty_path() {
        let bundle_content = format!(
            "{}\n\
            {}\n\
            {}\n\
            Content",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX, FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(&result, BundleValidationError::EmptyPath { line_number: 2 });
    }

    #[test]
    fn test_error_absolute_path() {
        let absolute_path_str = "/an/absolute/path.txt";
        let bundle_content = format!(
            "{}\n\
            {}{}\n\
            {}\n\
            Content",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX, absolute_path_str, FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::AbsolutePathNotAllowed {
                line_number: 2,
                path: absolute_path_str.to_string(),
            },
        );
    }

    #[test]
    fn test_error_duplicate_path() {
        let bundle_content = format!(
            "{}\n\
            {}file.txt\n\
            {}\n\
            Content1\n\
            {}\n\
            {}file.txt\n\
            {}\n\
            Content2",
            FILE_HEADER_SEPARATOR,
            FILE_PATH_PREFIX,
            FILE_HEADER_SEPARATOR,
            FILE_HEADER_SEPARATOR,
            FILE_PATH_PREFIX,
            FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::DuplicatePath {
                line_number: 6,
                path: "file.txt".to_string(),
            },
        );
    }

    #[test]
    fn test_error_premature_eof_after_first_separator() {
        let bundle_content = FILE_HEADER_SEPARATOR;
        let temp_file = create_temp_bundle_file(bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::PrematureEOFBeforePathLine { line_number: 1 },
        );
    }

    #[test]
    fn test_error_premature_eof_after_file_prefix() {
        let bundle_content = format!("{}\n{}", FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX);
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::MalformedHeaderPathLineMissingNewline {
                line_number: 2,
                path_line: "".to_string(),
            },
        );
    }

    #[test]
    fn test_error_premature_eof_after_path_line() {
        let bundle_content = format!("{}\n{}path.txt", FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX);
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert_specific_error(
            &result,
            BundleValidationError::MalformedHeaderPathLineMissingNewline {
                line_number: 2,
                path_line: "path.txt".to_string(),
            },
        );
    }

    #[test]
    fn test_error_unexpected_content_after_last_entry() {
        let bundle_content = format!(
            "{}\n\
            {}file.txt\n\
            {}\n\
            Content\n\
            Some trailing garbage text.",
            FILE_HEADER_SEPARATOR, FILE_PATH_PREFIX, FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());
        assert!(result.is_ok(), "Expected Ok, got {:?}", result);
        let entries = result.unwrap();
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].path, PathBuf::from("file.txt"));
        assert_eq!(entries[0].content, "Content\nSome trailing garbage text.");
    }

    #[test]
    fn test_multiple_errors_reported() {
        let bundle_content = format!(
            "Leading garbage.\n\
            {}\n\
            {}/abs/path.txt\n\
            {}\n\
            Content1\n\
            {}\n\
            {}\n\
            {}\n\
            Content2\n\
            Trailing garbage.",
            FILE_HEADER_SEPARATOR,
            FILE_PATH_PREFIX,
            FILE_HEADER_SEPARATOR,
            FILE_HEADER_SEPARATOR,
            FILE_PATH_PREFIX,
            FILE_HEADER_SEPARATOR
        );
        let temp_file = create_temp_bundle_file(&bundle_content);
        let result = parse_bundle(temp_file.path());

        assert!(result.is_err());
        if let Err(err) = result {
            if let Some(bundle_parse_error) = err.downcast_ref::<BundleParseError>() {
                assert_eq!(
                    bundle_parse_error.errors.len(),
                    2,
                    "Expected 2 errors, got {}. Errors: {:?}",
                    bundle_parse_error.errors.len(),
                    bundle_parse_error.errors
                );

                assert!(
                    !bundle_parse_error.errors.contains(
                        &BundleValidationError::ContentBeforeFirstHeader {
                            line_number: 1,
                            content_excerpt: "Leading garbage.".to_string()
                        }
                    ),
                    "ContentBeforeFirstHeader should now be a warning, not an error."
                );

                assert!(bundle_parse_error.errors.contains(
                    &BundleValidationError::AbsolutePathNotAllowed {
                        line_number: 3,
                        path: "/abs/path.txt".to_string()
                    }
                ));
                assert!(
                    bundle_parse_error
                        .errors
                        .contains(&BundleValidationError::EmptyPath { line_number: 7 })
                );
            } else {
                panic!("Error is not a BundleParseError: {:?}", err);
            }
        } else {
            panic!("Expected an error, but got Ok. Result: {:?}", result);
        }
    }
}



================================================
File: tests/integration_tests.rs
================================================
use assert_cmd::prelude::*;
use predicates::prelude::*;
use std::fs;
use std::io::Write;
use std::process::Command;
use tempfile::NamedTempFile;
use tempfile::TempDir;

fn create_temp_bundle_file(content: &str) -> NamedTempFile {
    let mut file = NamedTempFile::new().expect("Failed to create temp bundle file");
    write!(file, "{}", content).expect("Failed to write to temp bundle file");
    file
}

#[test]
fn test_valid_bundle_simple_structure() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: file1.txt\n================================================\nHello from file1\n================================================\nFile: file2.txt\n================================================\nContent of file2\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path()).arg(output_dir.path());

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Successfully sprouted 2 file(s) from '{}' to '{}'.",
            bundle_file.path().display(),
            output_dir.path().display()
        )));

    let file1_path = output_dir.path().join("file1.txt");
    let file2_path = output_dir.path().join("file2.txt");

    assert!(file1_path.exists());
    assert_eq!(fs::read_to_string(file1_path)?, "Hello from file1\n");

    assert!(file2_path.exists());
    assert_eq!(fs::read_to_string(file2_path)?, "Content of file2\n");

    Ok(())
}

#[test]
fn test_valid_bundle_nested_structure() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: dir1/file1.txt\n================================================\nNested content\n================================================\nFile: dir1/dir2/file2.txt\n================================================\nDeeply nested\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path()).arg(output_dir.path());

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Successfully sprouted 2 file(s) from '{}' to '{}'.",
            bundle_file.path().display(),
            output_dir.path().display()
        )));

    let file1_path = output_dir.path().join("dir1/file1.txt");
    let file2_path = output_dir.path().join("dir1/dir2/file2.txt");

    assert!(file1_path.exists());
    assert_eq!(fs::read_to_string(file1_path)?, "Nested content\n");

    assert!(file2_path.exists());
    assert_eq!(fs::read_to_string(file2_path)?, "Deeply nested\n");

    Ok(())
}

#[test]
fn test_malformed_bundle_file() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: file1.txt\nThis is not a valid header\nContent\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path()).arg(output_dir.path());

    cmd.assert().failure().stderr(
        predicate::str::contains("Bundle parsing failed")
            .and(predicate::str::contains("Malformed file header")),
    );

    assert!(fs::read_dir(output_dir.path())?.next().is_none());

    Ok(())
}

#[test]
fn test_output_file_collision() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: existing_file.txt\n================================================\nSome content\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;

    let conflicting_file_path = output_dir.path().join("existing_file.txt");
    fs::write(&conflicting_file_path, "Original content")?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path()).arg(output_dir.path());

    cmd.assert().failure().stderr(
        predicate::str::contains("Output path collision detected").and(predicate::str::contains(
            conflicting_file_path.to_str().unwrap(),
        )),
    );

    assert_eq!(
        fs::read_to_string(conflicting_file_path)?,
        "Original content"
    );

    Ok(())
}

#[test]
fn test_empty_bundle_file() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_file = create_temp_bundle_file("");
    let output_dir = TempDir::new()?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path()).arg(output_dir.path());

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Bundle file '{}' is empty or contains no valid entries. Nothing to sprout.",
            bundle_file.path().display()
        )));

    assert!(fs::read_dir(output_dir.path())?.next().is_none());
    Ok(())
}

#[test]
fn test_output_to_current_directory_default() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: default_dir_file.txt\n================================================\nDefault dir test\n";
    let bundle_file = create_temp_bundle_file(bundle_content);

    let current_dir_scope = TempDir::new()?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.current_dir(current_dir_scope.path())
        .arg(bundle_file.path());

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Successfully sprouted 1 file(s) from '{}' to '{}'.",
            bundle_file.path().display(),
            "."
        )));

    let file_path = current_dir_scope.path().join("default_dir_file.txt");
    assert!(file_path.exists());
    assert_eq!(fs::read_to_string(file_path)?, "Default dir test\n");

    Ok(())
}

#[test]
fn test_output_to_specified_directory_via_positional_arg() -> Result<(), Box<dyn std::error::Error>>
{
    let bundle_content = "================================================\nFile: specified_pos_file.txt\n================================================\nSpecified dir test - positional\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path()).arg(output_dir.path());

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Successfully sprouted 1 file(s) from '{}' to '{}'.",
            bundle_file.path().display(),
            output_dir.path().display()
        )));

    let file_path = output_dir.path().join("specified_pos_file.txt");
    assert!(file_path.exists());
    assert_eq!(
        fs::read_to_string(file_path)?,
        "Specified dir test - positional\n"
    );

    Ok(())
}

#[test]
fn test_output_to_specified_directory_via_o_flag() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: specified_flag_file.txt\n================================================\nSpecified dir test - flag\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path()).arg("-o").arg(output_dir.path());

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Successfully sprouted 1 file(s) from '{}' to '{}'.",
            bundle_file.path().display(),
            output_dir.path().display()
        )));

    let file_path = output_dir.path().join("specified_flag_file.txt");
    assert!(file_path.exists());
    assert_eq!(
        fs::read_to_string(file_path)?,
        "Specified dir test - flag\n"
    );

    Ok(())
}

#[test]
fn test_input_via_i_flag() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: input_flag_test.txt\n================================================\nInput via -i flag\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg("-i")
        .arg(bundle_file.path())
        .arg("-o")
        .arg(output_dir.path());

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Successfully sprouted 1 file(s) from '{}' to '{}'.",
            bundle_file.path().display(),
            output_dir.path().display()
        )));

    let file_path = output_dir.path().join("input_flag_test.txt");
    assert!(file_path.exists());
    assert_eq!(fs::read_to_string(file_path)?, "Input via -i flag\n");

    Ok(())
}

#[test]
fn test_missing_input_bundle() -> Result<(), Box<dyn std::error::Error>> {
    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.assert().failure().stderr(
        predicate::str::contains("error: the following required arguments were not provided:")
            .and(predicate::str::contains("<BUNDLE_FILE_PATH>")),
    );
    Ok(())
}

#[test]
fn test_bundle_with_empty_file_content() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: empty_file.txt\n================================================\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path()).arg(output_dir.path());

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Successfully sprouted 1 file(s) from '{}' to '{}'.",
            bundle_file.path().display(),
            output_dir.path().display()
        )));

    let file_path = output_dir.path().join("empty_file.txt");
    assert!(file_path.exists());
    assert_eq!(fs::read_to_string(file_path)?, "");

    Ok(())
}

#[test]
fn test_force_overwrite_existing_file() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: overwrite_me.txt\n================================================\nNew Content\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;
    let target_file_path = output_dir.path().join("overwrite_me.txt");

    // Create the file initially
    fs::write(&target_file_path, "Old Content")?;
    assert_eq!(fs::read_to_string(&target_file_path)?, "Old Content");

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path())
        .arg(output_dir.path())
        .arg("--force");

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Successfully sprouted 1 file(s) from '{}' to '{}'. (files overwritten if necessary)",
            bundle_file.path().display(),
            output_dir.path().display()
        )));

    assert!(target_file_path.exists());
    assert_eq!(fs::read_to_string(target_file_path)?, "New Content\n");

    Ok(())
}

#[test]
fn test_force_overwrite_existing_file_short_flag() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: overwrite_me_short.txt\n================================================\nNew Content Short\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;
    let target_file_path = output_dir.path().join("overwrite_me_short.txt");

    // Create the file initially
    fs::write(&target_file_path, "Old Content Short")?;
    assert_eq!(fs::read_to_string(&target_file_path)?, "Old Content Short");

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path()).arg(output_dir.path()).arg("-f"); // Short flag for force

    cmd.assert()
        .success()
        .stdout(predicate::str::contains(format!(
            "Successfully sprouted 1 file(s) from '{}' to '{}'. (files overwritten if necessary)",
            bundle_file.path().display(),
            output_dir.path().display()
        )));

    assert!(target_file_path.exists());
    assert_eq!(fs::read_to_string(target_file_path)?, "New Content Short\n");

    Ok(())
}

#[test]
fn test_force_still_fails_if_parent_is_file() -> Result<(), Box<dyn std::error::Error>> {
    let bundle_content = "================================================\nFile: existing_file_as_parent/new_child.txt\n================================================\nShould not be created\n";
    let bundle_file = create_temp_bundle_file(bundle_content);
    let output_dir = TempDir::new()?;

    // Create a file that would be a parent directory
    let conflicting_parent_path = output_dir.path().join("existing_file_as_parent");
    fs::write(&conflicting_parent_path, "I am a file.")?;

    let mut cmd = Command::cargo_bin("sprout")?;
    cmd.arg(bundle_file.path())
        .arg(output_dir.path())
        .arg("--force");

    cmd.assert().failure().stderr(
        predicate::str::contains("Failed to create parent directory")
            .or(
                // Error from create_dir_all
                predicate::str::contains("its parent")
                    .and(predicate::str::contains("is an existing file")), // Error from bundler.rs explicit check
            )
            .or(
                predicate::str::contains("Failed to write file"), // Error from fs::write if parent is a file
            ),
    );

    // Ensure original file is untouched and no new file/directory was created under/as it
    assert_eq!(
        fs::read_to_string(&conflicting_parent_path)?,
        "I am a file."
    );
    assert!(
        !output_dir
            .path()
            .join("existing_file_as_parent/new_child.txt")
            .exists()
    );
    assert!(conflicting_parent_path.is_file());

    Ok(())
}




================================================
File: .github/CODEOWNERS
================================================
# This is a CODEOWNERS file.
#
# Lines starting with '#' are comments.
# Each line is a file pattern followed by one or more owners.
# These patterns follow the same rules as .gitignore.
#
# Owners can be:
# - A GitHub username (e.g., @octocat)
# - A GitHub team name (e.g., @my-org/my-team)
# - An email address (e.g., user@example.com)
#
# Order matters: the last matching pattern takes the most precedence.
#
# Example:
# *       @global-owner1 @global-owner2  # All files are owned by these users/teams
# *.js    @js-owner                      # .js files are owned by @js-owner
# /docs/  docs@example.com               # Files in the /docs/ directory are owned by docs@example.com

# All files in the repository are owned by nightconcept
* @nightconcept dark@nightconcept.net

# You can add more specific rules below if needed.
# For example, if different parts of the project have different owners:
#
# /cmd/      @nightconcept
# /internal/ @nightconcept
# /scripts/  dark@nightconcept.net



================================================
File: .github/copilot-instructions.md
================================================
# AI Project Guidelines (Condensed)

**Objective:** Define mandatory process, coding, testing, and interaction standards for AI assistance.

## 1. Preparation

* **Project Context (Session Start):** ALWAYS review key project docs: `docs/PRD.md` (architecture, goals, tech stack, versions, structure, style guide), `docs/digest.txt` (current state summary), `docs/TASKS.md` (assignments).
* **Task Prep (Before Work):**
    * ALWAYS consult `docs/TASKS.md` for your assignment. If missing, add it (concise description, `YYYY-MM-DD`).
    * ALWAYS review relevant existing code *before* suggesting changes.

## 2. Implementation Planning

**Present this plan before providing code for a task:**

* Problem description (brief).
* Solution overview (high-level).
* Implementation steps (list).
* Risks/Challenges (foreseen).

## 3. Development Workflow

* **Plan First:** Present plan (Sec 2) before coding.
* **Focus:** Target the specific task from `TASKS.md`. No unrelated refactoring unless tasked.
* **Modification Approach:**
    * Prioritize minimal, incremental, clean, elegant, idiomatic changes.
    * Explain significant suggestions (Sec 5.4).
    * Propose beneficial low-risk refactoring.
    * Avoid duplication; use helpers/modules.
    * Explain use of language strengths/pitfalls if relevant.
* **Dependencies:** No new/updated external dependencies without explicit maintainer approval (check `docs/PRD.md` for approved stack/versions). Use only approved dependencies.
* **Commits (User Task):** Follow Conventional Commits (`https://www.conventionalcommits.org/en/v1.0.0/`).
* **Manual Testing:** Provide clear user instructions for manually testing the task's changes.

## 4. Folder Structure

* **Strict Adherence:** Follow structure defined in `docs/PRD.md`.
* **Changes:** No adding/removing/relocating files/dirs without prior maintainer approval. Approved structure changes require updating `docs/PRD.md` *before* implementation.
* **Source Location:** All source code must be in `src/`.
* **Precedence:** This rule is foundational.

## 5. Coding Standards

### 5.1. General & Robustness

* Follow language best practices unless overridden by `docs/PRD.md` or these guidelines.
* Prioritize: Clarity, maintainability, efficiency.
* Consider performance & basic security.
* Implement robust error handling (language norms or `PRD.md` spec); handle errors gracefully.

### 5.2. Modularity & Structure

* Keep files focused (ideally < 500 lines); refactor large ones.
* Prefer small, single-purpose functions.
* Structure code logically (per `docs/PRD.md`) into modules.
* Use clear, consistent imports (relative for local packages). Verify paths.

### 5.3. Style & Formatting

* **Priority:** 1) `docs/PRD.md`, 2) These rules, 3) Language common practices.
* **Type Hinting:** Mandatory for functions/classes/modules (dynamic languages).
* **Indentation:** 2 spaces.
* **Function Calls:** No space: `func()` not `func ()`.
* **Line Structure:** Avoid collapsing statements if clarity suffers.
* **Scope:** Default local. More descriptive names for wider scope. Avoid single-letter vars (except iterators/tiny scope; `i` only for loops). Use `_` for ignored vars.
* **Casing:** Match current file style; else language common style. `UPPER_CASE` for constants only.
* **Booleans:** Prefer `is_` prefix for boolean functions.
* **File Headers:** Top comment: Title (descriptive, not filename) + brief purpose. No version/OS info.

### 5.4. Documentation & Comments

* **Docstrings:** Required for public functions, classes, modules (standard format).
* **Code Comments:** Explain non-obvious logic, complex algorithms, decisions (*why*, not *what*).
* **Reasoning Comments:** Use `# Reason:` for complex block rationale.
* **README Updates:** Update `docs/README.md` for core features, dependency changes, or setup/build modifications.

## 6. Testing

* **Goal:** Tests are living documentation specifying behavior. Use common language framework.
* **Behavior Specification:** Tests specify behavior. Type/scope/timing (e.g., E2E, Unit, Integration) defined in `docs/PRD.md` per project phase.
* **Location:** Place tests in `/src/test` (Lua: `/src/spec`), mirroring `src/` structure (Sec 4).
    * Ex: Tests for `src/engine/mod.js` -> `src/test/engine/mod_test.js`.
    * Ex: Lua spec for `src/engine/mod.lua` -> `src/spec/engine/mod_spec.lua`.
* **Content:** Tests clearly describe expected behavior per `PRD.md` goals for the current phase.
    * **Prototype Phase:** Primary focus on automated E2E tests validating core functionality.
* **Strategy & Coverage:** Defined in `PRD.md`, evolves with phases.
    * **Prototype Phase:** E2E priority. Comprehensive unit tests & code coverage metrics (e.g., 100% statement coverage) are **not** the focus *unless* specified in `docs/PRD.md` for a later phase demanding them.
* **Updating Tests:** Review/update tests with code changes to reflect *current* expected behavior. Fix failing/outdated tests promptly.

## 7. AI Interaction Protocols

### 7.1. Engineering Role & Audience

* **Role:** Act as a **Senior Software Engineer**.
* **Audience:** Target **Mid-Level Software Engineers** (code = best practices, clear, documented; explanations thorough; justify complex choices).

### 7.2. Interaction Guidelines

* Ask clarifying questions if needed; do not assume.
* Verify facts (libs, APIs, file paths); do not invent. Use MCP servers if available.
* Do not delete/overwrite code unless instructed or part of the defined task.
* Report significant blockers/errors *during* implementation promptly with context and suggestions.
* If a task seems complex, state potential benefit from a more advanced model **boldly** at the start (e.g., "**Suggestion: This complex refactoring might benefit from a more advanced model.**").
* Be friendly, helpful, collaborative.
* Explicitly state when task requirements are met. Mark task complete in `docs/TASKS.md`.



================================================
File: .github/dependabot.yml
================================================
version: 2
updates:
  - package-ecosystem: github-actions
    directory: /
    schedule:
      interval: daily

  - package-ecosystem: cargo
    directory: /
    schedule:
      interval: daily



================================================
File: .github/workflows/ci.yml
================================================
name: Rust CI

permissions:
  contents: read
  security-events: read
  actions: read

on:
  push:
    branches: [main]
    tags: [ 'v*' ]
  pull_request:
    branches: [main]

jobs:
  build_and_test:
    name: Test on Rust ${{ matrix.rust-version }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        rust-version: [stable] # You can add more versions like 'beta', 'nightly', or specific versions '1.70.0'

    steps:
      - name: Harden the runner (Audit all outbound calls)
        uses: step-security/harden-runner@0634a2670c59f64b4a01f0f96f84700a4088b9f0
        with:
          egress-policy: audit # Configure as needed, 'audit' is a good start

      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
        with:
          fetch-depth: 2

      - name: Set up Rust toolchain
        uses: dtolnay/rust-toolchain@4305c38b25d97ef35a8ad1f985ccf2d2242004f2
        with:
          toolchain: ${{ matrix.rust-version }}
          components: clippy, rustfmt

      - name: Cache Cargo dependencies
        uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/            # Cache the target directory to speed up subsequent builds
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Install additional Rust tools
        run: |
          cargo install cargo-audit --force
          cargo install cargo-tarpaulin --version 0.32.7 --force

      - name: Run security audit
        run: cargo audit

      - name: Check formatting
        run: cargo fmt --all -- --check

      - name: Lint code
        run: cargo clippy --all-targets --all-features -- -D warnings

      - name: Run tests
        run: cargo test --all-features --verbose

      - name: Generate code coverage (Tarpaulin)
        run: |
          cargo tarpaulin --verbose --all-features --workspace --engine Llvm --out Xml --output-dir target/tarpaulin
          # Tarpaulin by default creates cobertura.xml in the output directory

      - name: Upload coverage to Coveralls
        uses: coverallsapp/github-action@648a8eb78e6d50909eff900e4ec85cab4524a45b # v2.3.6
        if: matrix.rust-version == 'stable'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          file: target/tarpaulin/cobertura.xml



================================================
File: .github/workflows/codeql.yml
================================================
# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: "CodeQL"

on:
  push:
    branches: ["main"]
  pull_request:
    # The branches below must be a subset of the branches above
    branches: ["main"]
  schedule:
    - cron: "0 0 * * 1"

permissions:
  contents: read

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: ["python"]
        # CodeQL supports [ $supported-codeql-languages ]
        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support

    steps:
      - name: Harden the runner (Audit all outbound calls)
        uses: step-security/harden-runner@0634a2670c59f64b4a01f0f96f84700a4088b9f0 # v2.12.0
        with:
          egress-policy: audit

      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      # Initializes the CodeQL tools for scanning.
      - name: Initialize CodeQL
        uses: github/codeql-action/init@ff0a06e83cb2de871e5a09832bc6a81e7276941f # v3.28.18
        with:
          languages: ${{ matrix.language }}
          # If you wish to specify custom queries, you can do so here or in a config file.
          # By default, queries listed here will override any specified in a config file.
          # Prefix the list here with "+" to use these queries and those in the config file.

      # Autobuild attempts to build any compiled languages  (C/C++, C#, or Java).
      # If this step fails, then you should remove it and run the build manually (see below)
      - name: Autobuild
        uses: github/codeql-action/autobuild@ff0a06e83cb2de871e5a09832bc6a81e7276941f # v3.28.18

      # ‚ÑπÔ∏è Command-line programs to run using the OS shell.
      # üìö See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun

      #   If the Autobuild fails above, remove it and uncomment the following three lines.
      #   modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.

      # - run: |
      #   echo "Run, Build Application using script"
      #   ./location_of_script_within_repo/buildscript.sh

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@ff0a06e83cb2de871e5a09832bc6a81e7276941f # v3.28.18
        with:
          category: "/language:${{matrix.language}}"



================================================
File: .github/workflows/dependency-review.yml
================================================
# Dependency Review Action
#
# This Action will scan dependency manifest files that change as part of a Pull Request,
# surfacing known-vulnerable versions of the packages declared or updated in the PR.
# Once installed, if the workflow run is marked as required,
# PRs introducing known-vulnerable packages will be blocked from merging.
#
# Source repository: https://github.com/actions/dependency-review-action
name: 'Dependency Review'
on: [pull_request]

permissions:
  contents: read

jobs:
  dependency-review:
    runs-on: ubuntu-latest
    steps:
      - name: Harden the runner (Audit all outbound calls)
        uses: step-security/harden-runner@0634a2670c59f64b4a01f0f96f84700a4088b9f0 # v2.12.0
        with:
          egress-policy: audit

      - name: 'Checkout Repository'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - name: 'Dependency Review'
        uses: actions/dependency-review-action@da24556b548a50705dd671f47852072ea4c105d9 # v4.7.1



================================================
File: .github/workflows/release.yml
================================================
name: Release Please

on:
  push:
    branches:
      - main

permissions:
  contents: write
  pull-requests: write

jobs:
  release-please:
    runs-on: ubuntu-latest
    steps:
      - name: Harden the runner (Audit all outbound calls)
        uses: step-security/harden-runner@0634a2670c59f64b4a01f0f96f84700a4088b9f0 # v2.12.0
        with:
          egress-policy: audit

      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Release Please
        id: release
        uses: googleapis/release-please-action@a02a34c4d625f9be7cb89156071d8567266a2445
        with:
          release-type: rust
          # package-name: sprout # This input is deprecated for release-type: rust
          # Optional: Define a custom token if needed, though default GITHUB_TOKEN often works.
          token: ${{ secrets.GITHUB_TOKEN }}
          # Optional: If you want release-please to create a PR instead of releasing directly
          # command: release-pr
          # Optional: If you want to include a manifest file (e.g. for cargo-dist)
          # manifest-file: .manifest.json

  # This job runs after a release has been created by release-please
  # (either directly or after a release PR is merged).
  # It builds the binaries and uploads them as release assets.
  build-and-upload-assets:
    needs: release-please
    if: ${{ needs.release-please.outputs.release_created == 'true' }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
            asset_name_suffix: linux-x86_64
            binary_name: sprout
          - os: macos-latest
            target: x86_64-apple-darwin
            asset_name_suffix: macos-x86_64
            binary_name: sprout
          - os: macos-latest
            target: aarch64-apple-darwin
            asset_name_suffix: macos-aarch64
            binary_name: sprout
          - os: windows-latest
            target: x86_64-pc-windows-msvc
            asset_name_suffix: windows-x86_64
            binary_name: sprout.exe
    steps:
      - name: Harden the runner (Audit all outbound calls)
        uses: step-security/harden-runner@0634a2670c59f64b4a01f0f96f84700a4088b9f0 # v2.12.0
        with:
          egress-policy: audit

      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          # Checkout the specific tag created by release-please
          ref: ${{ needs.release-please.outputs.tag_name }}

      - name: Set up Rust
        uses: dtolnay/rust-toolchain@4305c38b25d97ef35a8ad1f985ccf2d2242004f2 # stable
        with:
          toolchain: stable
          targets: ${{ matrix.target }}

      - name: Build binary
        run: cargo build --release --target ${{ matrix.target }}
        env:
          CARGO_TERM_COLOR: always

      - name: Prepare asset name
        id: asset_details
        run: |
          VERSION_TAG="${{ needs.release-please.outputs.tag_name }}"
          # Remove 'v' prefix if present (e.g., v0.1.0 -> 0.1.0)
          VERSION="${VERSION_TAG#v}"
          ASSET_NAME="sprout-v${VERSION}-${{ matrix.asset_name_suffix }}"
          echo "ASSET_NAME=${ASSET_NAME}" >> $GITHUB_OUTPUT
          echo "VERSION=${VERSION}" >> $GITHUB_OUTPUT


      - name: Upload Release Asset (Linux/macOS)
        if: runner.os != 'Windows'
        uses: actions/upload-release-asset@e8f9f06c4b078e705bd2ea027f0926603fc9b4d5 # v1.0.2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.release-please.outputs.upload_url }}
          asset_path: ./target/${{ matrix.target }}/release/${{ matrix.binary_name }}
          asset_name: ${{ steps.asset_details.outputs.ASSET_NAME }}
          asset_content_type: application/octet-stream

      - name: Upload Release Asset (Windows)
        if: runner.os == 'Windows'
        uses: actions/upload-release-asset@e8f9f06c4b078e705bd2ea027f0926603fc9b4d5 # v1.0.2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.release-please.outputs.upload_url }}
          asset_path: ./target/${{ matrix.target }}/release/${{ matrix.binary_name }}
          asset_name: ${{ steps.asset_details.outputs.ASSET_NAME }}.exe # Ensure .exe for Windows asset name if binary_name doesn't include it
          asset_content_type: application/octet-stream


================================================
File: .github/workflows/scorecards.yml
================================================
# This workflow uses actions that are not certified by GitHub. They are provided
# by a third-party and are governed by separate terms of service, privacy
# policy, and support documentation.

name: Scorecard supply-chain security
on:
  # For Branch-Protection check. Only the default branch is supported. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#branch-protection
  branch_protection_rule:
  # To guarantee Maintained check is occasionally updated. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#maintained
  schedule:
    - cron: '20 7 * * 2'
  push:
    branches: ["main"]

# Declare default permissions as read only.

jobs:
  analysis:
    name: Scorecard analysis
    runs-on: ubuntu-latest
    permissions:
      # Needed to upload the results to code-scanning dashboard.
      security-events: write
      # Needed to publish results and get a badge (see publish_results below).
      id-token: write
      contents: read
      actions: read
      # To allow GraphQL ListCommits to work
      issues: read
      pull-requests: read
      # To detect SAST tools
      checks: read

    steps:
      - name: Harden the runner (Audit all outbound calls)
        uses: step-security/harden-runner@0634a2670c59f64b4a01f0f96f84700a4088b9f0 # v2.12.0
        with:
          egress-policy: audit

      - name: "Checkout code"
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          persist-credentials: false

      - name: "Run analysis"
        uses: ossf/scorecard-action@f49aabe0b5af0936a0987cfb85d86b75731b0186 # v2.4.1
        with:
          results_file: results.sarif
          results_format: sarif
          # (Optional) "write" PAT token. Uncomment the `repo_token` line below if:
          # - you want to enable the Branch-Protection check on a *public* repository, or
          # - you are installing Scorecards on a *private* repository
          # To create the PAT, follow the steps in https://github.com/ossf/scorecard-action#authentication-with-pat.
          # repo_token: ${{ secrets.SCORECARD_TOKEN }}

          # Public repositories:
          #   - Publish results to OpenSSF REST API for easy access by consumers
          #   - Allows the repository to include the Scorecard badge.
          #   - See https://github.com/ossf/scorecard-action#publishing-results.
          # For private repositories:
          #   - `publish_results` will always be set to `false`, regardless
          #     of the value entered here.
          publish_results: true

      # Upload the results as artifacts (optional). Commenting out will disable uploads of run results in SARIF
      # format to the repository Actions tab.
      - name: "Upload artifact"
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: SARIF file
          path: results.sarif
          retention-days: 5

      # Upload the results to GitHub's code scanning dashboard.
      - name: "Upload to code-scanning"
        uses: github/codeql-action/upload-sarif@ff0a06e83cb2de871e5a09832bc6a81e7276941f # v3.28.18
        with:
          sarif_file: results.sarif



